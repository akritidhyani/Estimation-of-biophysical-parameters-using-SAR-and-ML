{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "AbQ2verbn02V",
        "outputId": "4b72ae11-61f1-4b1b-b5bb-a998166d6d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation-rmse:0.31677\n",
            "[1]\tvalidation-rmse:0.31567\n",
            "[2]\tvalidation-rmse:0.31164\n",
            "[3]\tvalidation-rmse:0.31661\n",
            "[4]\tvalidation-rmse:0.31981\n",
            "[5]\tvalidation-rmse:0.31745\n",
            "[6]\tvalidation-rmse:0.31660\n",
            "[7]\tvalidation-rmse:0.31489\n",
            "[8]\tvalidation-rmse:0.31454\n",
            "[9]\tvalidation-rmse:0.31311\n",
            "[10]\tvalidation-rmse:0.31747\n",
            "[11]\tvalidation-rmse:0.32170\n",
            "[12]\tvalidation-rmse:0.32580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:25:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"n_estimators\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Mean Squared Error: 0.031431848336362304\n",
            "Testing Mean Squared Error: 0.11154501479713709\n",
            "Training Mean Absolute Error: 0.14404010348849827\n",
            "Testing Mean Absolute Error: 0.2660386987527212\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/File.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "# Keep only the VV and VH columns as features\n",
        "X = data[['VV_mean', 'VH_mean']]\n",
        "y = data['LAI']\n",
        "\n",
        "# Splitting the data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert to DMatrix format\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',  # For regression\n",
        "    'max_depth': 3,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 100,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(dtrain)\n",
        "test_predictions = model.predict(dtest)\n",
        "\n",
        "# Calculate training and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error: {train_mse}')\n",
        "print(f'Testing Mean Squared Error: {test_mse}')\n",
        "print(f'Training Mean Absolute Error: {train_mae}')\n",
        "print(f'Testing Mean Absolute Error: {test_mae}')\n",
        "\n",
        "# Visualize feature importance\n",
        "xgb.plot_importance(model)\n",
        "plt.title(\"Feature Importance\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: algorithm that predicts LAI on the basis of VV and VH. Train the model and also test and validate it\n",
        "\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/File.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "# Keep only the VV and VH columns as features\n",
        "X = data[['VV_mean', 'VH_mean']]\n",
        "y = data['LAI']\n",
        "\n",
        "# Splitting the data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert to DMatrix format\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',  # For regression\n",
        "    'max_depth': 3,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 100,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(dtrain)\n",
        "valid_predictions = model.predict(dvalid)\n",
        "test_predictions = model.predict(dtest)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error: {train_mse}')\n",
        "print(f'Validation Mean Squared Error: {valid_mse}')\n",
        "print(f'Testing Mean Squared Error: {test_mse}')\n",
        "print(f'Training Mean Absolute Error: {train_mae}')\n",
        "print(f'Validation Mean Absolute Error: {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error: {test_mae}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivtOF5abrwbF",
        "outputId": "bc3e3935-5d20-4675-c5c3-a1dc4a6f3e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation-rmse:0.31677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [05:52:03] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"n_estimators\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\tvalidation-rmse:0.31567\n",
            "[2]\tvalidation-rmse:0.31164\n",
            "[3]\tvalidation-rmse:0.31661\n",
            "[4]\tvalidation-rmse:0.31981\n",
            "[5]\tvalidation-rmse:0.31745\n",
            "[6]\tvalidation-rmse:0.31660\n",
            "[7]\tvalidation-rmse:0.31489\n",
            "[8]\tvalidation-rmse:0.31454\n",
            "[9]\tvalidation-rmse:0.31311\n",
            "[10]\tvalidation-rmse:0.31747\n",
            "[11]\tvalidation-rmse:0.32170\n",
            "[12]\tvalidation-rmse:0.32580\n",
            "Training Mean Squared Error: 0.031431848336362304\n",
            "Validation Mean Squared Error: 0.10614356050953094\n",
            "Testing Mean Squared Error: 0.11154501479713709\n",
            "Training Mean Absolute Error: 0.14404010348849827\n",
            "Validation Mean Absolute Error: 0.25830315311749774\n",
            "Testing Mean Absolute Error: 0.2660386987527212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: try another algorithm\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/File.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "# Keep only the VV and VH columns as features\n",
        "X = data[['VV_mean', 'VH_mean']]\n",
        "y = data['LAI']\n",
        "\n",
        "# Splitting the data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Create and train the Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = rf_model.predict(X_train)\n",
        "valid_predictions = rf_model.predict(X_valid)\n",
        "test_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (Random Forest): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (Random Forest): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (Random Forest): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')\n",
        "\n",
        "# You can also analyze feature importance for Random Forest\n",
        "importances = rf_model.feature_importances_\n",
        "feature_names = X.columns\n",
        "for feature, importance in zip(feature_names, importances):\n",
        "    print(f\"{feature}: {importance}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK5rEFBLsf9b",
        "outputId": "caf2cd9c-c3dc-44a5-9960-e1a6e15ad8c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Mean Squared Error (Random Forest): 0.012425486805555533\n",
            "Validation Mean Squared Error (Random Forest): 0.14210521750000024\n",
            "Testing Mean Squared Error (Random Forest): 0.11528069833333358\n",
            "Training Mean Absolute Error (Random Forest): 0.09340833333333329\n",
            "Validation Mean Absolute Error (Random Forest): 0.30848333333333355\n",
            "Testing Mean Absolute Error (Random Forest): 0.27518333333333383\n",
            "VV_mean: 0.48421731653708755\n",
            "VH_mean: 0.5157826834629125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: use ANN\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/File.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "# Keep only the VV and VH columns as features\n",
        "X = data[['VV_mean', 'VH_mean']]\n",
        "y = data['LAI']\n",
        "\n",
        "# Splitting the data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Create the ANN model\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Mean Absolute Error: {mae}')\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(X_train).flatten()\n",
        "valid_predictions = model.predict(X_valid).flatten()\n",
        "test_predictions = model.predict(X_test).flatten()\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (ANN): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (ANN): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (ANN): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (ANN): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (ANN): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (ANN): {test_mae}')\n",
        "\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZVQbSMydsyhd",
        "outputId": "7e8c75ba-93df-4611-bae3-585d077538f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 387ms/step - loss: 0.9279 - mae: 0.9204 - val_loss: 0.1520 - val_mae: 0.3256\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.1160 - mae: 0.2821 - val_loss: 0.2451 - val_mae: 0.3960\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.2015 - mae: 0.3690 - val_loss: 0.2911 - val_mae: 0.4361\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 0.2455 - mae: 0.4251 - val_loss: 0.1703 - val_mae: 0.3212\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.1150 - mae: 0.2671 - val_loss: 0.1238 - val_mae: 0.2834\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 0.0703 - mae: 0.2253 - val_loss: 0.1836 - val_mae: 0.3553\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 0.1453 - mae: 0.3203 - val_loss: 0.2519 - val_mae: 0.4287\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.2035 - mae: 0.3863 - val_loss: 0.2131 - val_mae: 0.3828\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1648 - mae: 0.3377 - val_loss: 0.1346 - val_mae: 0.3051\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.0895 - mae: 0.2543 - val_loss: 0.1321 - val_mae: 0.2834\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.0847 - mae: 0.2275 - val_loss: 0.1792 - val_mae: 0.3281\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1349 - mae: 0.2923 - val_loss: 0.2011 - val_mae: 0.3484\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1495 - mae: 0.3134 - val_loss: 0.1635 - val_mae: 0.3158\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.1142 - mae: 0.2622 - val_loss: 0.1238 - val_mae: 0.2768\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0797 - mae: 0.2349 - val_loss: 0.1310 - val_mae: 0.3001\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 0.0864 - mae: 0.2446 - val_loss: 0.1375 - val_mae: 0.3095\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0929 - mae: 0.2528 - val_loss: 0.1353 - val_mae: 0.3067\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - loss: 0.0915 - mae: 0.2529 - val_loss: 0.1317 - val_mae: 0.3014\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0879 - mae: 0.2485 - val_loss: 0.1256 - val_mae: 0.2890\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.0754 - mae: 0.2326 - val_loss: 0.1227 - val_mae: 0.2760\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0774 - mae: 0.2341 - val_loss: 0.1317 - val_mae: 0.2825\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.0842 - mae: 0.2280 - val_loss: 0.1421 - val_mae: 0.2938\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0933 - mae: 0.2402 - val_loss: 0.1376 - val_mae: 0.2879\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.0854 - mae: 0.2275 - val_loss: 0.1241 - val_mae: 0.2758\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0768 - mae: 0.2259 - val_loss: 0.1225 - val_mae: 0.2798\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0748 - mae: 0.2329 - val_loss: 0.1237 - val_mae: 0.2844\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.0776 - mae: 0.2370 - val_loss: 0.1243 - val_mae: 0.2868\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - loss: 0.0774 - mae: 0.2372 - val_loss: 0.1220 - val_mae: 0.2786\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0733 - mae: 0.2313 - val_loss: 0.1258 - val_mae: 0.2758\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0780 - mae: 0.2241 - val_loss: 0.1426 - val_mae: 0.2935\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0990 - mae: 0.2459 - val_loss: 0.1536 - val_mae: 0.3051\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.1094 - mae: 0.2573 - val_loss: 0.1426 - val_mae: 0.2935\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0913 - mae: 0.2357 - val_loss: 0.1239 - val_mae: 0.2743\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0739 - mae: 0.2207 - val_loss: 0.1297 - val_mae: 0.3008\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0856 - mae: 0.2437 - val_loss: 0.1478 - val_mae: 0.3230\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.1035 - mae: 0.2641 - val_loss: 0.1353 - val_mae: 0.3092\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0913 - mae: 0.2507 - val_loss: 0.1216 - val_mae: 0.2789\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0753 - mae: 0.2346 - val_loss: 0.1247 - val_mae: 0.2743\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0783 - mae: 0.2266 - val_loss: 0.1239 - val_mae: 0.2742\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0735 - mae: 0.2214 - val_loss: 0.1216 - val_mae: 0.2746\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0771 - mae: 0.2349 - val_loss: 0.1216 - val_mae: 0.2751\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0742 - mae: 0.2307 - val_loss: 0.1233 - val_mae: 0.2740\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0773 - mae: 0.2272 - val_loss: 0.1257 - val_mae: 0.2754\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0768 - mae: 0.2218 - val_loss: 0.1217 - val_mae: 0.2745\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0735 - mae: 0.2302 - val_loss: 0.1263 - val_mae: 0.2946\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0801 - mae: 0.2396 - val_loss: 0.1366 - val_mae: 0.3113\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0912 - mae: 0.2494 - val_loss: 0.1297 - val_mae: 0.3012\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0834 - mae: 0.2443 - val_loss: 0.1216 - val_mae: 0.2747\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0766 - mae: 0.2325 - val_loss: 0.1323 - val_mae: 0.2825\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0860 - mae: 0.2307 - val_loss: 0.1344 - val_mae: 0.2843\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0841 - mae: 0.2272 - val_loss: 0.1217 - val_mae: 0.2740\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0760 - mae: 0.2304 - val_loss: 0.1309 - val_mae: 0.3027\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.0861 - mae: 0.2447 - val_loss: 0.1354 - val_mae: 0.3091\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.0908 - mae: 0.2489 - val_loss: 0.1240 - val_mae: 0.2886\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0764 - mae: 0.2346 - val_loss: 0.1220 - val_mae: 0.2738\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0754 - mae: 0.2284 - val_loss: 0.1275 - val_mae: 0.2778\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0812 - mae: 0.2271 - val_loss: 0.1253 - val_mae: 0.2750\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0761 - mae: 0.2232 - val_loss: 0.1218 - val_mae: 0.2800\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0739 - mae: 0.2318 - val_loss: 0.1306 - val_mae: 0.3022\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.0806 - mae: 0.2365 - val_loss: 0.1274 - val_mae: 0.2964\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0766 - mae: 0.2372 - val_loss: 0.1226 - val_mae: 0.2739\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0774 - mae: 0.2303 - val_loss: 0.1342 - val_mae: 0.2841\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0888 - mae: 0.2334 - val_loss: 0.1275 - val_mae: 0.2778\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0782 - mae: 0.2243 - val_loss: 0.1215 - val_mae: 0.2740\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 0.0714 - mae: 0.2255 - val_loss: 0.1225 - val_mae: 0.2836\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0766 - mae: 0.2365 - val_loss: 0.1272 - val_mae: 0.2968\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0822 - mae: 0.2412 - val_loss: 0.1285 - val_mae: 0.2992\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0813 - mae: 0.2394 - val_loss: 0.1237 - val_mae: 0.2881\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0760 - mae: 0.2340 - val_loss: 0.1214 - val_mae: 0.2760\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0728 - mae: 0.2276 - val_loss: 0.1236 - val_mae: 0.2738\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0742 - mae: 0.2235 - val_loss: 0.1246 - val_mae: 0.2739\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.0795 - mae: 0.2298 - val_loss: 0.1217 - val_mae: 0.2739\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.0749 - mae: 0.2291 - val_loss: 0.1223 - val_mae: 0.2828\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 0.0714 - mae: 0.2277 - val_loss: 0.1219 - val_mae: 0.2804\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0760 - mae: 0.2354 - val_loss: 0.1226 - val_mae: 0.2738\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0739 - mae: 0.2243 - val_loss: 0.1334 - val_mae: 0.2834\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0848 - mae: 0.2271 - val_loss: 0.1290 - val_mae: 0.2794\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0850 - mae: 0.2325 - val_loss: 0.1216 - val_mae: 0.2792\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0743 - mae: 0.2336 - val_loss: 0.1273 - val_mae: 0.2967\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0816 - mae: 0.2402 - val_loss: 0.1285 - val_mae: 0.2988\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0825 - mae: 0.2413 - val_loss: 0.1216 - val_mae: 0.2794\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0756 - mae: 0.2333 - val_loss: 0.1227 - val_mae: 0.2738\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0777 - mae: 0.2309 - val_loss: 0.1253 - val_mae: 0.2750\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0781 - mae: 0.2258 - val_loss: 0.1310 - val_mae: 0.2812\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0855 - mae: 0.2329 - val_loss: 0.1355 - val_mae: 0.2850\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0904 - mae: 0.2354 - val_loss: 0.1290 - val_mae: 0.2792\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0821 - mae: 0.2262 - val_loss: 0.1214 - val_mae: 0.2749\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0770 - mae: 0.2353 - val_loss: 0.1237 - val_mae: 0.2883\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0772 - mae: 0.2355 - val_loss: 0.1226 - val_mae: 0.2845\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0721 - mae: 0.2294 - val_loss: 0.1265 - val_mae: 0.2762\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0803 - mae: 0.2255 - val_loss: 0.1422 - val_mae: 0.2922\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0931 - mae: 0.2375 - val_loss: 0.1313 - val_mae: 0.2812\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.0840 - mae: 0.2307 - val_loss: 0.1215 - val_mae: 0.2762\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0755 - mae: 0.2328 - val_loss: 0.1301 - val_mae: 0.3031\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0853 - mae: 0.2451 - val_loss: 0.1373 - val_mae: 0.3134\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0916 - mae: 0.2506 - val_loss: 0.1323 - val_mae: 0.3067\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0832 - mae: 0.2441 - val_loss: 0.1217 - val_mae: 0.2776\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0745 - mae: 0.2304 - val_loss: 0.1370 - val_mae: 0.2856\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0907 - mae: 0.2352 - val_loss: 0.1575 - val_mae: 0.3074\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.1113 - mae: 0.2620 - val_loss: 0.1424 - val_mae: 0.2919\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1633 - mae: 0.3444\n",
            "Test Loss: 0.1632528454065323\n",
            "Test Mean Absolute Error: 0.3443543016910553\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Training Mean Squared Error (ANN): 0.09604122044068569\n",
            "Validation Mean Squared Error (ANN): 0.14241442498998322\n",
            "Testing Mean Squared Error (ANN): 0.16325285049891827\n",
            "Training Mean Absolute Error (ANN): 0.24156509293450248\n",
            "Validation Mean Absolute Error (ANN): 0.2918612039089203\n",
            "Testing Mean Absolute Error (ANN): 0.3443543144067129\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: try using svm\n",
        "\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/File.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "# Keep only the VV and VH columns as features\n",
        "X = data[['VV_mean', 'VH_mean']]\n",
        "y = data['LAI']\n",
        "\n",
        "# Splitting the data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Create and train the SVM model\n",
        "svm_model = SVR(kernel='rbf')  # You can experiment with different kernels\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = svm_model.predict(X_train)\n",
        "valid_predictions = svm_model.predict(X_valid)\n",
        "test_predictions = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (SVM): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (SVM): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (SVM): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (SVM): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (SVM): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (SVM): {test_mae}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "849DUSWbtP7s",
        "outputId": "e9e9ebe1-603f-4eba-8b4f-390adec50307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Mean Squared Error (SVM): 0.06755406297155214\n",
            "Validation Mean Squared Error (SVM): 0.10429861300330619\n",
            "Testing Mean Squared Error (SVM): 0.09890588452525316\n",
            "Training Mean Absolute Error (SVM): 0.21541319319264385\n",
            "Validation Mean Absolute Error (SVM): 0.24411527670286293\n",
            "Testing Mean Absolute Error (SVM): 0.2602112006365664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "kxVOBoGwyeH6",
        "outputId": "6560d3bb-22ba-4d43-af22-a92237fe8425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     LAI  Plant_heig  DBH_m_  canopy_rad  canopy_cir  ndvi2023_mean  \\\n",
              "0  1.660        3.40     0.8         3.5        30.0       0.472324   \n",
              "1  1.000        0.00     0.0         0.0        22.0       0.339296   \n",
              "2  1.370        6.00     0.6         3.6        22.0       0.431640   \n",
              "3  1.130        4.31     0.7         3.7        12.0       0.401281   \n",
              "4  1.275       10.00     4.2         0.0        34.0       0.354280   \n",
              "\n",
              "   ndre2023_mean  lswi2023_mean  Evi2023_mean    VV_mean  ...  LAI.1  \\\n",
              "0       0.379955       0.292221      6.562868 -11.313682  ...   1.66   \n",
              "1       0.246918       0.168713     -9.171651  -8.771105  ...    NaN   \n",
              "2       0.333228       0.239454      9.557211 -10.699364  ...   1.37   \n",
              "3       0.301087       0.200579      9.366930 -10.368002  ...   1.13   \n",
              "4       0.265736       0.178864     35.265920  -9.046082  ...    NaN   \n",
              "\n",
              "   Evi2023_mean.1  Unnamed: 37  LAI.2  Evi2023_mean.2  LAI.3   fdc  \\\n",
              "0        6.562868          NaN   1.66         2.90000  0.800  1.60   \n",
              "1             NaN          NaN    NaN             NaN  1.030  2.06   \n",
              "2        9.557211          NaN   1.37         3.00000  1.135  2.27   \n",
              "3        9.366930          NaN   1.13         9.36693  1.170  2.34   \n",
              "4             NaN          NaN    NaN             NaN  1.200  2.40   \n",
              "\n",
              "   Unnamed: 42  Unnamed: 43 Unnamed: 44  \n",
              "0     0.029559          NaN    0.059118  \n",
              "1     0.126633          NaN    0.253265  \n",
              "2     0.157004          NaN    0.314007  \n",
              "3     0.182322          NaN    0.364643  \n",
              "4     0.190620          NaN    0.381241  \n",
              "\n",
              "[5 rows x 45 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8aabe446-e823-4a37-87a4-eea77814bfe1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LAI</th>\n",
              "      <th>Plant_heig</th>\n",
              "      <th>DBH_m_</th>\n",
              "      <th>canopy_rad</th>\n",
              "      <th>canopy_cir</th>\n",
              "      <th>ndvi2023_mean</th>\n",
              "      <th>ndre2023_mean</th>\n",
              "      <th>lswi2023_mean</th>\n",
              "      <th>Evi2023_mean</th>\n",
              "      <th>VV_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>LAI.1</th>\n",
              "      <th>Evi2023_mean.1</th>\n",
              "      <th>Unnamed: 37</th>\n",
              "      <th>LAI.2</th>\n",
              "      <th>Evi2023_mean.2</th>\n",
              "      <th>LAI.3</th>\n",
              "      <th>fdc</th>\n",
              "      <th>Unnamed: 42</th>\n",
              "      <th>Unnamed: 43</th>\n",
              "      <th>Unnamed: 44</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.660</td>\n",
              "      <td>3.40</td>\n",
              "      <td>0.8</td>\n",
              "      <td>3.5</td>\n",
              "      <td>30.0</td>\n",
              "      <td>0.472324</td>\n",
              "      <td>0.379955</td>\n",
              "      <td>0.292221</td>\n",
              "      <td>6.562868</td>\n",
              "      <td>-11.313682</td>\n",
              "      <td>...</td>\n",
              "      <td>1.66</td>\n",
              "      <td>6.562868</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.66</td>\n",
              "      <td>2.90000</td>\n",
              "      <td>0.800</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.029559</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.059118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.339296</td>\n",
              "      <td>0.246918</td>\n",
              "      <td>0.168713</td>\n",
              "      <td>-9.171651</td>\n",
              "      <td>-8.771105</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.030</td>\n",
              "      <td>2.06</td>\n",
              "      <td>0.126633</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.253265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.370</td>\n",
              "      <td>6.00</td>\n",
              "      <td>0.6</td>\n",
              "      <td>3.6</td>\n",
              "      <td>22.0</td>\n",
              "      <td>0.431640</td>\n",
              "      <td>0.333228</td>\n",
              "      <td>0.239454</td>\n",
              "      <td>9.557211</td>\n",
              "      <td>-10.699364</td>\n",
              "      <td>...</td>\n",
              "      <td>1.37</td>\n",
              "      <td>9.557211</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.37</td>\n",
              "      <td>3.00000</td>\n",
              "      <td>1.135</td>\n",
              "      <td>2.27</td>\n",
              "      <td>0.157004</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.314007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.130</td>\n",
              "      <td>4.31</td>\n",
              "      <td>0.7</td>\n",
              "      <td>3.7</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.401281</td>\n",
              "      <td>0.301087</td>\n",
              "      <td>0.200579</td>\n",
              "      <td>9.366930</td>\n",
              "      <td>-10.368002</td>\n",
              "      <td>...</td>\n",
              "      <td>1.13</td>\n",
              "      <td>9.366930</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.13</td>\n",
              "      <td>9.36693</td>\n",
              "      <td>1.170</td>\n",
              "      <td>2.34</td>\n",
              "      <td>0.182322</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.364643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.275</td>\n",
              "      <td>10.00</td>\n",
              "      <td>4.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.354280</td>\n",
              "      <td>0.265736</td>\n",
              "      <td>0.178864</td>\n",
              "      <td>35.265920</td>\n",
              "      <td>-9.046082</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.200</td>\n",
              "      <td>2.40</td>\n",
              "      <td>0.190620</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.381241</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 45 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8aabe446-e823-4a37-87a4-eea77814bfe1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8aabe446-e823-4a37-87a4-eea77814bfe1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8aabe446-e823-4a37-87a4-eea77814bfe1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0380d6ae-3bec-4dc2-a185-c8eb60fa3fe4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0380d6ae-3bec-4dc2-a185-c8eb60fa3fe4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0380d6ae-3bec-4dc2-a185-c8eb60fa3fe4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: provide the same code to predict LAI with input variables ndvi and ndre from the file\n",
        "\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/File.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "# Keep only the ndvi and ndre columns as features\n",
        "X = data[['ndvi2023_mean', 'ndre2023_mean']]\n",
        "y = data['LAI']\n",
        "\n",
        "# Splitting the data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# XGBoost\n",
        "# Convert to DMatrix format\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',  # For regression\n",
        "    'max_depth': 3,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 100,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(dtrain)\n",
        "valid_predictions = model.predict(dvalid)\n",
        "test_predictions = model.predict(dtest)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (XGBoost): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (XGBoost): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (XGBoost): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (XGBoost): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (XGBoost): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (XGBoost): {test_mae}')\n",
        "\n",
        "\n",
        "# Random Forest\n",
        "# Create and train the Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = rf_model.predict(X_train)\n",
        "valid_predictions = rf_model.predict(X_valid)\n",
        "test_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (Random Forest): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (Random Forest): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (Random Forest): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')\n",
        "\n",
        "\n",
        "# ANN\n",
        "# Create the ANN model\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Mean Absolute Error: {mae}')\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(X_train).flatten()\n",
        "valid_predictions = model.predict(X_valid).flatten()\n",
        "test_predictions = model.predict(X_test).flatten()\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (ANN): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (ANN): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (ANN): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (ANN): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (ANN): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (ANN): {test_mae}')\n",
        "\n",
        "\n",
        "# SVM\n",
        "# Create and train the SVM model\n",
        "svm_model = SVR(kernel='rbf')  # You can experiment with different kernels\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = svm_model.predict(X_train)\n",
        "valid_predictions = svm_model.predict(X_valid)\n",
        "test_predictions = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (SVM): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (SVM): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (SVM): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (SVM): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (SVM): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (SVM): {test_mae}')\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZRSd_bAE4CS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['VV_mean', 'VH_mean']]\n",
        "y = data['DBH_m_']\n",
        "\n",
        "# Splitting the data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# XGBoost\n",
        "# Convert to DMatrix format\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',  # For regression\n",
        "    'max_depth': 3,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 100,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(dtrain)\n",
        "valid_predictions = model.predict(dvalid)\n",
        "test_predictions = model.predict(dtest)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (XGBoost): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (XGBoost): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (XGBoost): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (XGBoost): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (XGBoost): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (XGBoost): {test_mae}')\n",
        "\n",
        "\n",
        "# Random Forest\n",
        "# Create and train the Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = rf_model.predict(X_train)\n",
        "valid_predictions = rf_model.predict(X_valid)\n",
        "test_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (Random Forest): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (Random Forest): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (Random Forest): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')\n",
        "\n",
        "\n",
        "# ANN\n",
        "# Create the ANN model\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Mean Absolute Error: {mae}')\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(X_train).flatten()\n",
        "valid_predictions = model.predict(X_valid).flatten()\n",
        "test_predictions = model.predict(X_test).flatten()\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (ANN): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (ANN): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (ANN): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (ANN): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (ANN): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (ANN): {test_mae}')\n",
        "\n",
        "\n",
        "# SVM\n",
        "# Create and train the SVM model\n",
        "svm_model = SVR(kernel='rbf')  # You can experiment with different kernels\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = svm_model.predict(X_train)\n",
        "valid_predictions = svm_model.predict(X_valid)\n",
        "test_predictions = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (SVM): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (SVM): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (SVM): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (SVM): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (SVM): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (SVM): {test_mae}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0gjy837YLoZS",
        "outputId": "d1f9337f-d5a7-4e40-9314-fbfed6700fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation-rmse:1.88149\n",
            "[1]\tvalidation-rmse:1.88398\n",
            "[2]\tvalidation-rmse:1.87340\n",
            "[3]\tvalidation-rmse:1.87034\n",
            "[4]\tvalidation-rmse:1.86956\n",
            "[5]\tvalidation-rmse:1.91533\n",
            "[6]\tvalidation-rmse:1.89850\n",
            "[7]\tvalidation-rmse:1.87041\n",
            "[8]\tvalidation-rmse:1.85601\n",
            "[9]\tvalidation-rmse:1.83150\n",
            "[10]\tvalidation-rmse:1.86399\n",
            "[11]\tvalidation-rmse:1.86149\n",
            "[12]\tvalidation-rmse:1.85003\n",
            "[13]\tvalidation-rmse:1.82947\n",
            "[14]\tvalidation-rmse:1.85797\n",
            "[15]\tvalidation-rmse:1.86801\n",
            "[16]\tvalidation-rmse:1.86476\n",
            "[17]\tvalidation-rmse:1.88963\n",
            "[18]\tvalidation-rmse:1.90252\n",
            "[19]\tvalidation-rmse:1.90581\n",
            "[20]\tvalidation-rmse:1.90957\n",
            "[21]\tvalidation-rmse:1.91849\n",
            "[22]\tvalidation-rmse:1.91659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [02:27:34] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"n_estimators\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Mean Squared Error (XGBoost): 0.6336688611474042\n",
            "Validation Mean Squared Error (XGBoost): 3.6754689789448354\n",
            "Testing Mean Squared Error (XGBoost): 3.3383812252223692\n",
            "Training Mean Absolute Error (XGBoost): 0.6264812529087068\n",
            "Validation Mean Absolute Error (XGBoost): 1.4730230808258058\n",
            "Testing Mean Absolute Error (XGBoost): 1.5229980568091073\n",
            "Training Mean Squared Error (Random Forest): 0.30094719444444484\n",
            "Validation Mean Squared Error (Random Forest): 4.57434225\n",
            "Testing Mean Squared Error (Random Forest): 3.0131444166666674\n",
            "Training Mean Absolute Error (Random Forest): 0.45108333333333367\n",
            "Validation Mean Absolute Error (Random Forest): 1.632083333333333\n",
            "Testing Mean Absolute Error (Random Forest): 1.4624166666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 251ms/step - loss: 4.4656 - mae: 1.8130 - val_loss: 3.4664 - val_mae: 1.4453\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 3.0420 - mae: 1.5293 - val_loss: 3.0493 - val_mae: 1.3062\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 3.5833 - mae: 1.4987 - val_loss: 3.0747 - val_mae: 1.3193\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3.5225 - mae: 1.4593 - val_loss: 3.0568 - val_mae: 1.2918\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3.0628 - mae: 1.4258 - val_loss: 3.4541 - val_mae: 1.4402\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 3.0313 - mae: 1.5265 - val_loss: 3.9888 - val_mae: 1.6084\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 3.0268 - mae: 1.5394 - val_loss: 3.9003 - val_mae: 1.5855\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3.0718 - mae: 1.5558 - val_loss: 3.7176 - val_mae: 1.5343\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.0342 - mae: 1.5420 - val_loss: 3.5334 - val_mae: 1.4729\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.9733 - mae: 1.5188 - val_loss: 3.3801 - val_mae: 1.4122\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 3.0123 - mae: 1.5152 - val_loss: 3.3009 - val_mae: 1.3750\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3.0019 - mae: 1.4991 - val_loss: 3.1803 - val_mae: 1.3307\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 3.0636 - mae: 1.4915 - val_loss: 3.0765 - val_mae: 1.2983\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 3.1281 - mae: 1.4625 - val_loss: 3.0756 - val_mae: 1.2979\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 3.0621 - mae: 1.4588 - val_loss: 3.2212 - val_mae: 1.3405\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.8184 - mae: 1.4505 - val_loss: 3.6825 - val_mae: 1.5205\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.9015 - mae: 1.5100 - val_loss: 4.4889 - val_mae: 1.7248\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 3.2553 - mae: 1.5724 - val_loss: 5.0038 - val_mae: 1.8250\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3.6858 - mae: 1.6647 - val_loss: 4.8607 - val_mae: 1.7983\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3.5461 - mae: 1.6249 - val_loss: 4.4619 - val_mae: 1.7180\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.2324 - mae: 1.5678 - val_loss: 4.0828 - val_mae: 1.6302\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.0856 - mae: 1.5557 - val_loss: 3.7617 - val_mae: 1.5419\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.9632 - mae: 1.5232 - val_loss: 3.4713 - val_mae: 1.4421\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.9070 - mae: 1.4949 - val_loss: 3.3340 - val_mae: 1.3818\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 3.0328 - mae: 1.5200 - val_loss: 3.3244 - val_mae: 1.3771\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 2.9712 - mae: 1.4914 - val_loss: 3.3071 - val_mae: 1.3712\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.8939 - mae: 1.4747 - val_loss: 3.3164 - val_mae: 1.3745\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3.0048 - mae: 1.5007 - val_loss: 3.3207 - val_mae: 1.3760\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.0166 - mae: 1.5132 - val_loss: 3.3231 - val_mae: 1.3769\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.8806 - mae: 1.4808 - val_loss: 3.3910 - val_mae: 1.4049\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.8769 - mae: 1.4881 - val_loss: 3.4684 - val_mae: 1.4370\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.8971 - mae: 1.4984 - val_loss: 3.5526 - val_mae: 1.4687\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.9030 - mae: 1.5053 - val_loss: 3.6624 - val_mae: 1.5063\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.9923 - mae: 1.5350 - val_loss: 3.6513 - val_mae: 1.5023\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.8039 - mae: 1.4843 - val_loss: 3.4739 - val_mae: 1.4374\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 2.9287 - mae: 1.5024 - val_loss: 3.3519 - val_mae: 1.3870\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.8539 - mae: 1.4634 - val_loss: 3.3656 - val_mae: 1.3915\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 2.9713 - mae: 1.4996 - val_loss: 3.3418 - val_mae: 1.3838\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 2.8961 - mae: 1.4893 - val_loss: 3.2793 - val_mae: 1.3615\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 3.0021 - mae: 1.4943 - val_loss: 3.2605 - val_mae: 1.3548\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.9182 - mae: 1.4727 - val_loss: 3.2943 - val_mae: 1.3674\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.8663 - mae: 1.4640 - val_loss: 3.4093 - val_mae: 1.4061\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.9444 - mae: 1.4947 - val_loss: 3.4314 - val_mae: 1.4139\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.7804 - mae: 1.4587 - val_loss: 3.5114 - val_mae: 1.4457\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.8031 - mae: 1.4786 - val_loss: 3.7114 - val_mae: 1.5152\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.9538 - mae: 1.5194 - val_loss: 3.6987 - val_mae: 1.5102\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.9386 - mae: 1.5169 - val_loss: 3.4608 - val_mae: 1.4224\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.9046 - mae: 1.4976 - val_loss: 3.3432 - val_mae: 1.3856\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 2.8694 - mae: 1.4623 - val_loss: 3.3257 - val_mae: 1.3795\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.8155 - mae: 1.4519 - val_loss: 3.2554 - val_mae: 1.3571\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.9680 - mae: 1.4738 - val_loss: 3.2027 - val_mae: 1.3442\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 2.9744 - mae: 1.4650 - val_loss: 3.2058 - val_mae: 1.3452\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.9709 - mae: 1.4575 - val_loss: 3.3616 - val_mae: 1.3923\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 2.8839 - mae: 1.4678 - val_loss: 3.6103 - val_mae: 1.4744\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.9336 - mae: 1.5108 - val_loss: 3.6657 - val_mae: 1.4931\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.8206 - mae: 1.4833 - val_loss: 3.5433 - val_mae: 1.4481\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.8627 - mae: 1.4911 - val_loss: 3.4479 - val_mae: 1.4203\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.8090 - mae: 1.4630 - val_loss: 3.3953 - val_mae: 1.4039\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.8240 - mae: 1.4582 - val_loss: 3.3492 - val_mae: 1.3881\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.7953 - mae: 1.4437 - val_loss: 3.3034 - val_mae: 1.3706\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 2.9187 - mae: 1.4694 - val_loss: 3.2754 - val_mae: 1.3643\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 2.8469 - mae: 1.4455 - val_loss: 3.2882 - val_mae: 1.3674\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 2.8982 - mae: 1.4666 - val_loss: 3.3550 - val_mae: 1.3902\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 2.8151 - mae: 1.4534 - val_loss: 3.5456 - val_mae: 1.4490\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 2.7425 - mae: 1.4675 - val_loss: 3.9550 - val_mae: 1.5757\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 3.0427 - mae: 1.5493 - val_loss: 4.4478 - val_mae: 1.6969\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 3.1852 - mae: 1.5596 - val_loss: 4.4952 - val_mae: 1.7069\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 3.1213 - mae: 1.5430 - val_loss: 3.9914 - val_mae: 1.5845\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.9600 - mae: 1.5283 - val_loss: 3.3743 - val_mae: 1.3971\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.8900 - mae: 1.4697 - val_loss: 3.1627 - val_mae: 1.3283\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.9834 - mae: 1.4329 - val_loss: 3.1454 - val_mae: 1.3181\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.1487 - mae: 1.4549 - val_loss: 3.1820 - val_mae: 1.3371\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.0818 - mae: 1.4680 - val_loss: 3.3063 - val_mae: 1.3734\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.8750 - mae: 1.4513 - val_loss: 3.4730 - val_mae: 1.4294\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 2.8447 - mae: 1.4790 - val_loss: 3.6468 - val_mae: 1.4765\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.7308 - mae: 1.4606 - val_loss: 3.9147 - val_mae: 1.5590\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 2.9444 - mae: 1.5170 - val_loss: 4.1154 - val_mae: 1.6134\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 2.9241 - mae: 1.5067 - val_loss: 3.7629 - val_mae: 1.5118\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 2.8410 - mae: 1.4962 - val_loss: 3.3923 - val_mae: 1.4032\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.7360 - mae: 1.4342 - val_loss: 3.2781 - val_mae: 1.3677\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.8530 - mae: 1.4434 - val_loss: 3.2816 - val_mae: 1.3687\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.8590 - mae: 1.4514 - val_loss: 3.3994 - val_mae: 1.4056\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.8901 - mae: 1.4729 - val_loss: 3.5245 - val_mae: 1.4450\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.7408 - mae: 1.4439 - val_loss: 3.4784 - val_mae: 1.4314\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 2.8331 - mae: 1.4672 - val_loss: 3.3132 - val_mae: 1.3765\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 2.9226 - mae: 1.4675 - val_loss: 3.2018 - val_mae: 1.3433\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 3.0400 - mae: 1.4539 - val_loss: 3.1893 - val_mae: 1.3372\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 3.0657 - mae: 1.4481 - val_loss: 3.2043 - val_mae: 1.3441\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 2.9201 - mae: 1.4309 - val_loss: 3.2651 - val_mae: 1.3648\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 2.9762 - mae: 1.4674 - val_loss: 3.4042 - val_mae: 1.4070\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 2.7936 - mae: 1.4464 - val_loss: 3.4853 - val_mae: 1.4337\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 2.7395 - mae: 1.4552 - val_loss: 3.6710 - val_mae: 1.4844\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 2.8166 - mae: 1.4741 - val_loss: 3.9812 - val_mae: 1.5713\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 2.9444 - mae: 1.5168 - val_loss: 3.9669 - val_mae: 1.5667\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 2.9096 - mae: 1.5074 - val_loss: 3.6286 - val_mae: 1.4741\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 2.8349 - mae: 1.4850 - val_loss: 3.3634 - val_mae: 1.3911\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.8929 - mae: 1.4703 - val_loss: 3.3076 - val_mae: 1.3765\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.8634 - mae: 1.4471 - val_loss: 3.3284 - val_mae: 1.3816\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 2.8987 - mae: 1.4574 - val_loss: 3.3545 - val_mae: 1.3875\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.7917 - mae: 1.4351 - val_loss: 3.4401 - val_mae: 1.4190\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 3.7207 - mae: 1.5225\n",
            "Test Loss: 3.7206687927246094\n",
            "Test Mean Absolute Error: 1.5225063562393188\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Training Mean Squared Error (ANN): 2.808844072262929\n",
            "Validation Mean Squared Error (ANN): 3.4400722835121553\n",
            "Testing Mean Squared Error (ANN): 3.7206692356078874\n",
            "Training Mean Absolute Error (ANN): 1.458646625942654\n",
            "Validation Mean Absolute Error (ANN): 1.4190051396687826\n",
            "Testing Mean Absolute Error (ANN): 1.5225064476331074\n",
            "Training Mean Squared Error (SVM): 2.2493518637216297\n",
            "Validation Mean Squared Error (SVM): 4.649900553486048\n",
            "Testing Mean Squared Error (SVM): 2.6431304008756205\n",
            "Training Mean Absolute Error (SVM): 1.189212847302958\n",
            "Validation Mean Absolute Error (SVM): 1.5606862212655352\n",
            "Testing Mean Absolute Error (SVM): 1.2848624426348956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['VH_mean', 'Plant_heig']]\n",
        "y = data['DBH_m_']\n",
        "\n",
        "# Splitting the data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# XGBoost\n",
        "# Convert to DMatrix format\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',  # For regression\n",
        "    'max_depth': 3,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 100,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(dtrain)\n",
        "valid_predictions = model.predict(dvalid)\n",
        "test_predictions = model.predict(dtest)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (XGBoost): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (XGBoost): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (XGBoost): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (XGBoost): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (XGBoost): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (XGBoost): {test_mae}')\n",
        "\n",
        "\n",
        "# Random Forest\n",
        "# Create and train the Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = rf_model.predict(X_train)\n",
        "valid_predictions = rf_model.predict(X_valid)\n",
        "test_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (Random Forest): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (Random Forest): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (Random Forest): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')\n",
        "\n",
        "# ANN\n",
        "# Create the ANN model\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Mean Absolute Error: {mae}')\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(X_train).flatten()\n",
        "valid_predictions = model.predict(X_valid).flatten()\n",
        "test_predictions = model.predict(X_test).flatten()\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (ANN): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (ANN): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (ANN): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (ANN): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (ANN): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (ANN): {test_mae}')\n",
        "\n",
        "\n",
        "# SVM\n",
        "# Create and train the SVM model\n",
        "svm_model = SVR(kernel='rbf')  # You can experiment with different kernels\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = svm_model.predict(X_train)\n",
        "valid_predictions = svm_model.predict(X_valid)\n",
        "test_predictions = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (SVM): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (SVM): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (SVM): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (SVM): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (SVM): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (SVM): {test_mae}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Hx-vHhABNpIb",
        "outputId": "f1e6b0a8-1df7-42b7-f0df-d396ab10013a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation-rmse:1.87181\n",
            "[1]\tvalidation-rmse:1.86367\n",
            "[2]\tvalidation-rmse:1.85231\n",
            "[3]\tvalidation-rmse:1.84382\n",
            "[4]\tvalidation-rmse:1.83544\n",
            "[5]\tvalidation-rmse:1.84321\n",
            "[6]\tvalidation-rmse:1.83847\n",
            "[7]\tvalidation-rmse:1.87503\n",
            "[8]\tvalidation-rmse:1.87209\n",
            "[9]\tvalidation-rmse:1.90624\n",
            "[10]\tvalidation-rmse:1.93907\n",
            "[11]\tvalidation-rmse:1.94615\n",
            "[12]\tvalidation-rmse:1.97067\n",
            "[13]\tvalidation-rmse:1.99652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [02:32:11] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"n_estimators\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Mean Squared Error (XGBoost): 0.7167571865685648\n",
            "Validation Mean Squared Error (XGBoost): 3.9638295138546247\n",
            "Testing Mean Squared Error (XGBoost): 3.6749045591640197\n",
            "Training Mean Absolute Error (XGBoost): 0.6886052714453803\n",
            "Validation Mean Absolute Error (XGBoost): 1.3460787256558737\n",
            "Testing Mean Absolute Error (XGBoost): 1.3648296395937602\n",
            "Training Mean Squared Error (Random Forest): 0.23965083333333365\n",
            "Validation Mean Squared Error (Random Forest): 3.5988265000000044\n",
            "Testing Mean Squared Error (Random Forest): 3.9741699999999986\n",
            "Training Mean Absolute Error (Random Forest): 0.3889444444444446\n",
            "Validation Mean Absolute Error (Random Forest): 1.3280000000000003\n",
            "Testing Mean Absolute Error (Random Forest): 1.3558333333333337\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 332ms/step - loss: 6.0965 - mae: 2.1681 - val_loss: 6.2732 - val_mae: 2.0569\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 4.0124 - mae: 1.7089 - val_loss: 4.6521 - val_mae: 1.6252\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 2.7219 - mae: 1.3800 - val_loss: 3.7653 - val_mae: 1.4391\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 2.0976 - mae: 1.1970 - val_loss: 3.2311 - val_mae: 1.3167\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.8256 - mae: 1.0753 - val_loss: 2.9762 - val_mae: 1.3005\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.8218 - mae: 1.0644 - val_loss: 2.9213 - val_mae: 1.2984\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.9989 - mae: 1.0870 - val_loss: 2.9845 - val_mae: 1.3280\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 2.1991 - mae: 1.1411 - val_loss: 3.0315 - val_mae: 1.3498\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 2.0275 - mae: 1.1040 - val_loss: 2.9901 - val_mae: 1.3240\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 2.0780 - mae: 1.1001 - val_loss: 2.9236 - val_mae: 1.2924\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.9458 - mae: 1.0661 - val_loss: 2.9499 - val_mae: 1.2932\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 1.8300 - mae: 1.0357 - val_loss: 3.0439 - val_mae: 1.2954\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 1.7881 - mae: 1.0310 - val_loss: 3.1323 - val_mae: 1.2953\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.7602 - mae: 1.0240 - val_loss: 3.1888 - val_mae: 1.2944\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.7679 - mae: 1.0327 - val_loss: 3.2023 - val_mae: 1.2937\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.7884 - mae: 1.0391 - val_loss: 3.1640 - val_mae: 1.2929\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.7103 - mae: 1.0070 - val_loss: 3.0852 - val_mae: 1.2954\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.7029 - mae: 1.0086 - val_loss: 3.0081 - val_mae: 1.2958\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.7385 - mae: 1.0161 - val_loss: 2.9741 - val_mae: 1.2952\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.7979 - mae: 1.0320 - val_loss: 2.9811 - val_mae: 1.2950\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.7991 - mae: 1.0332 - val_loss: 3.0050 - val_mae: 1.2951\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.7548 - mae: 1.0100 - val_loss: 3.0304 - val_mae: 1.2934\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.7092 - mae: 0.9856 - val_loss: 3.0838 - val_mae: 1.2927\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.5432 - mae: 0.9539 - val_loss: 3.1812 - val_mae: 1.2905\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.7089 - mae: 0.9923 - val_loss: 3.2599 - val_mae: 1.2891\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.7282 - mae: 0.9955 - val_loss: 3.2916 - val_mae: 1.2931\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.7699 - mae: 1.0060 - val_loss: 3.2987 - val_mae: 1.2933\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.7122 - mae: 0.9888 - val_loss: 3.2631 - val_mae: 1.2899\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 1.7011 - mae: 0.9693 - val_loss: 3.2110 - val_mae: 1.2916\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.7372 - mae: 0.9962 - val_loss: 3.1515 - val_mae: 1.2918\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.6937 - mae: 0.9759 - val_loss: 3.1283 - val_mae: 1.2918\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.6693 - mae: 0.9717 - val_loss: 3.1161 - val_mae: 1.2907\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.4583 - mae: 0.9042 - val_loss: 3.1455 - val_mae: 1.2903\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.6320 - mae: 0.9439 - val_loss: 3.2757 - val_mae: 1.2912\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.7217 - mae: 0.9791 - val_loss: 3.3795 - val_mae: 1.3055\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.7368 - mae: 0.9965 - val_loss: 3.3838 - val_mae: 1.3068\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.7167 - mae: 0.9917 - val_loss: 3.3226 - val_mae: 1.2939\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.7273 - mae: 0.9962 - val_loss: 3.2133 - val_mae: 1.2924\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.7002 - mae: 0.9741 - val_loss: 3.1035 - val_mae: 1.2905\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.6395 - mae: 0.9470 - val_loss: 3.0412 - val_mae: 1.2887\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.6915 - mae: 0.9453 - val_loss: 3.0222 - val_mae: 1.2867\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.6516 - mae: 0.9190 - val_loss: 3.0194 - val_mae: 1.2848\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.6334 - mae: 0.9161 - val_loss: 3.0262 - val_mae: 1.2841\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.6797 - mae: 0.9310 - val_loss: 3.0394 - val_mae: 1.2841\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.6580 - mae: 0.9134 - val_loss: 3.0527 - val_mae: 1.2848\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.6600 - mae: 0.9219 - val_loss: 3.0797 - val_mae: 1.2847\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.6843 - mae: 0.9430 - val_loss: 3.1334 - val_mae: 1.2829\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.6479 - mae: 0.9174 - val_loss: 3.1810 - val_mae: 1.2822\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.6438 - mae: 0.9191 - val_loss: 3.2016 - val_mae: 1.2827\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.6683 - mae: 0.9334 - val_loss: 3.1869 - val_mae: 1.2836\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.5972 - mae: 0.9026 - val_loss: 3.1838 - val_mae: 1.2854\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.6692 - mae: 0.9328 - val_loss: 3.2039 - val_mae: 1.2877\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.4925 - mae: 0.8971 - val_loss: 3.2285 - val_mae: 1.2899\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.4956 - mae: 0.9044 - val_loss: 3.2948 - val_mae: 1.2928\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.6023 - mae: 0.9272 - val_loss: 3.3296 - val_mae: 1.2977\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.7022 - mae: 0.9777 - val_loss: 3.2631 - val_mae: 1.3009\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.6857 - mae: 0.9801 - val_loss: 3.1969 - val_mae: 1.3017\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.6646 - mae: 0.9751 - val_loss: 3.1422 - val_mae: 1.3022\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.7011 - mae: 1.0021 - val_loss: 3.0811 - val_mae: 1.3015\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.6748 - mae: 0.9830 - val_loss: 3.0445 - val_mae: 1.3021\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.6195 - mae: 0.9478 - val_loss: 3.0195 - val_mae: 1.3021\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.7112 - mae: 0.9739 - val_loss: 3.0191 - val_mae: 1.3018\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.7281 - mae: 0.9657 - val_loss: 3.0335 - val_mae: 1.3013\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 1.4828 - mae: 0.9071 - val_loss: 3.0987 - val_mae: 1.3008\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 1.6289 - mae: 0.9396 - val_loss: 3.2815 - val_mae: 1.2987\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.6680 - mae: 0.9636 - val_loss: 3.4758 - val_mae: 1.3098\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.7451 - mae: 1.0106 - val_loss: 3.6004 - val_mae: 1.3346\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.7580 - mae: 1.0396 - val_loss: 3.5463 - val_mae: 1.3189\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.7843 - mae: 1.0437 - val_loss: 3.3875 - val_mae: 1.2926\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.6638 - mae: 0.9666 - val_loss: 3.2443 - val_mae: 1.2953\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 1.6231 - mae: 0.9359 - val_loss: 3.1026 - val_mae: 1.2984\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.6166 - mae: 0.9078 - val_loss: 3.0362 - val_mae: 1.2970\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1.6910 - mae: 0.9347 - val_loss: 3.0223 - val_mae: 1.3038\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.7139 - mae: 0.9433 - val_loss: 3.0121 - val_mae: 1.3044\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.7757 - mae: 0.9680 - val_loss: 3.0069 - val_mae: 1.2994\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1.7916 - mae: 0.9788 - val_loss: 3.0019 - val_mae: 1.2949\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.7398 - mae: 0.9506 - val_loss: 3.0038 - val_mae: 1.2939\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.6956 - mae: 0.9307 - val_loss: 3.0271 - val_mae: 1.2923\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 1.6583 - mae: 0.9182 - val_loss: 3.0882 - val_mae: 1.2904\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 1.6670 - mae: 0.9435 - val_loss: 3.1528 - val_mae: 1.2892\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.5617 - mae: 0.9060 - val_loss: 3.2205 - val_mae: 1.2888\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.6446 - mae: 0.9464 - val_loss: 3.3267 - val_mae: 1.2899\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.6698 - mae: 0.9698 - val_loss: 3.3957 - val_mae: 1.3072\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 1.7362 - mae: 1.0053 - val_loss: 3.4050 - val_mae: 1.3061\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.5800 - mae: 0.9631 - val_loss: 3.4719 - val_mae: 1.3188\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 1.5688 - mae: 0.9777 - val_loss: 3.5396 - val_mae: 1.3316\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 1.7873 - mae: 1.0471 - val_loss: 3.4895 - val_mae: 1.3164\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.7505 - mae: 1.0297 - val_loss: 3.3581 - val_mae: 1.2962\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.6485 - mae: 0.9652 - val_loss: 3.2437 - val_mae: 1.2965\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.6464 - mae: 0.9537 - val_loss: 3.1631 - val_mae: 1.2966\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.5691 - mae: 0.9038 - val_loss: 3.1251 - val_mae: 1.2961\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.6029 - mae: 0.9016 - val_loss: 3.1258 - val_mae: 1.2962\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.6325 - mae: 0.9118 - val_loss: 3.1424 - val_mae: 1.2950\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.6338 - mae: 0.9176 - val_loss: 3.1576 - val_mae: 1.2947\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.6282 - mae: 0.9189 - val_loss: 3.1802 - val_mae: 1.2916\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.5921 - mae: 0.9037 - val_loss: 3.2171 - val_mae: 1.2875\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.5609 - mae: 0.9025 - val_loss: 3.2698 - val_mae: 1.2847\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.4702 - mae: 0.9055 - val_loss: 3.3572 - val_mae: 1.2860\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.6821 - mae: 0.9789 - val_loss: 3.4762 - val_mae: 1.2923\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.7326 - mae: 1.0129 - val_loss: 3.5842 - val_mae: 1.3147\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 4.3921 - mae: 1.3213\n",
            "Test Loss: 4.392050266265869\n",
            "Test Mean Absolute Error: 1.3212989568710327\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Training Mean Squared Error (ANN): 1.747491461637295\n",
            "Validation Mean Squared Error (ANN): 3.5841587385338403\n",
            "Testing Mean Squared Error (ANN): 4.392050178605509\n",
            "Training Mean Absolute Error (ANN): 1.036638056404061\n",
            "Validation Mean Absolute Error (ANN): 1.3147137780984242\n",
            "Testing Mean Absolute Error (ANN): 1.3212990055481593\n",
            "Training Mean Squared Error (SVM): 1.7559782439466878\n",
            "Validation Mean Squared Error (SVM): 3.1098024086275533\n",
            "Testing Mean Squared Error (SVM): 3.3109083736444997\n",
            "Training Mean Absolute Error (SVM): 1.0642497027640883\n",
            "Validation Mean Absolute Error (SVM): 1.3089101302367236\n",
            "Testing Mean Absolute Error (SVM): 1.3528309083567283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['VH_mean', 'Plant_heig']]\n",
        "y = data['DBH_m_']\n",
        "\n",
        "# Splitting the data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# XGBoost\n",
        "# Convert to DMatrix format\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',  # For regression\n",
        "    'max_depth': 3,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 100,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(dtrain)\n",
        "valid_predictions = model.predict(dvalid)\n",
        "test_predictions = model.predict(dtest)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (XGBoost): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (XGBoost): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (XGBoost): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (XGBoost): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (XGBoost): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (XGBoost): {test_mae}')\n",
        "\n",
        "\n",
        "# Random Forest\n",
        "# Create and train the Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = rf_model.predict(X_train)\n",
        "valid_predictions = rf_model.predict(X_valid)\n",
        "test_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (Random Forest): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (Random Forest): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (Random Forest): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')\n",
        "\n",
        "\n",
        "# ANN\n",
        "# Create the ANN model\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Mean Absolute Error: {mae}')\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(X_train).flatten()\n",
        "valid_predictions = model.predict(X_valid).flatten()\n",
        "test_predictions = model.predict(X_test).flatten()\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (ANN): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (ANN): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (ANN): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (ANN): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (ANN): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (ANN): {test_mae}')\n",
        "\n",
        "\n",
        "# SVM\n",
        "# Create and train the SVM model\n",
        "svm_model = SVR(kernel='rbf')  # You can experiment with different kernels\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = svm_model.predict(X_train)\n",
        "valid_predictions = svm_model.predict(X_valid)\n",
        "test_predictions = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (SVM): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (SVM): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (SVM): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (SVM): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (SVM): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (SVM): {test_mae}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NreHTXZUQo6x",
        "outputId": "91d0dabe-b019-498c-b80a-5e24d9dbc142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation-rmse:1.87181\n",
            "[1]\tvalidation-rmse:1.86367\n",
            "[2]\tvalidation-rmse:1.85231\n",
            "[3]\tvalidation-rmse:1.84382\n",
            "[4]\tvalidation-rmse:1.83544\n",
            "[5]\tvalidation-rmse:1.84321\n",
            "[6]\tvalidation-rmse:1.83847\n",
            "[7]\tvalidation-rmse:1.87503\n",
            "[8]\tvalidation-rmse:1.87209\n",
            "[9]\tvalidation-rmse:1.90624\n",
            "[10]\tvalidation-rmse:1.93907\n",
            "[11]\tvalidation-rmse:1.94615\n",
            "[12]\tvalidation-rmse:1.97067\n",
            "[13]\tvalidation-rmse:1.99652\n",
            "[14]\tvalidation-rmse:1.99094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [02:44:16] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"n_estimators\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Mean Squared Error (XGBoost): 0.7167571865685648\n",
            "Validation Mean Squared Error (XGBoost): 3.9638295138546247\n",
            "Testing Mean Squared Error (XGBoost): 3.6749045591640197\n",
            "Training Mean Absolute Error (XGBoost): 0.6886052714453803\n",
            "Validation Mean Absolute Error (XGBoost): 1.3460787256558737\n",
            "Testing Mean Absolute Error (XGBoost): 1.3648296395937602\n",
            "Training Mean Squared Error (Random Forest): 0.23965083333333365\n",
            "Validation Mean Squared Error (Random Forest): 3.5988265000000044\n",
            "Testing Mean Squared Error (Random Forest): 3.9741699999999986\n",
            "Training Mean Absolute Error (Random Forest): 0.3889444444444446\n",
            "Validation Mean Absolute Error (Random Forest): 1.3280000000000003\n",
            "Testing Mean Absolute Error (Random Forest): 1.3558333333333337\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 287ms/step - loss: 2.2141 - mae: 1.2606 - val_loss: 3.2433 - val_mae: 1.3320\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1.8380 - mae: 1.1070 - val_loss: 3.2200 - val_mae: 1.3116\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.6961 - mae: 1.0222 - val_loss: 3.0635 - val_mae: 1.3009\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.7231 - mae: 1.0042 - val_loss: 2.9949 - val_mae: 1.2924\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.7288 - mae: 0.9546 - val_loss: 2.9884 - val_mae: 1.2841\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.7417 - mae: 0.9466 - val_loss: 2.9870 - val_mae: 1.2782\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - loss: 1.7202 - mae: 0.9154 - val_loss: 3.0039 - val_mae: 1.2742\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 1.7706 - mae: 0.9337 - val_loss: 3.0635 - val_mae: 1.2729\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.6657 - mae: 0.8952 - val_loss: 3.1369 - val_mae: 1.2737\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 1.7502 - mae: 0.9420 - val_loss: 3.1871 - val_mae: 1.2768\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 1.7241 - mae: 0.9434 - val_loss: 3.1532 - val_mae: 1.2809\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 1.6248 - mae: 0.9028 - val_loss: 3.0611 - val_mae: 1.2856\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1.6998 - mae: 0.9179 - val_loss: 3.0445 - val_mae: 1.2896\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 1.7225 - mae: 0.9192 - val_loss: 3.0548 - val_mae: 1.2905\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1.7113 - mae: 0.9168 - val_loss: 3.0761 - val_mae: 1.2930\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.6984 - mae: 0.9133 - val_loss: 3.0892 - val_mae: 1.2941\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 1.4863 - mae: 0.8733 - val_loss: 3.1258 - val_mae: 1.2970\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 1.3932 - mae: 0.8670 - val_loss: 3.3262 - val_mae: 1.3016\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1.7118 - mae: 1.0036 - val_loss: 3.6353 - val_mae: 1.3671\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1.8999 - mae: 1.1127 - val_loss: 3.6112 - val_mae: 1.3766\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 1.7544 - mae: 1.0461 - val_loss: 3.2864 - val_mae: 1.3144\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 1.6632 - mae: 0.9808 - val_loss: 3.0000 - val_mae: 1.3035\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 1.7081 - mae: 0.9723 - val_loss: 3.0372 - val_mae: 1.3349\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1.9592 - mae: 1.0446 - val_loss: 3.1341 - val_mae: 1.3777\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1.9851 - mae: 1.0287 - val_loss: 3.1078 - val_mae: 1.3538\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 1.9633 - mae: 1.0206 - val_loss: 3.0689 - val_mae: 1.3017\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 1.4546 - mae: 0.8710 - val_loss: 3.2410 - val_mae: 1.2961\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 1.6639 - mae: 0.9761 - val_loss: 3.6201 - val_mae: 1.3287\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1.8500 - mae: 1.0885 - val_loss: 3.7926 - val_mae: 1.3661\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 1.9174 - mae: 1.1159 - val_loss: 3.6474 - val_mae: 1.3389\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 1.8270 - mae: 1.0748 - val_loss: 3.2854 - val_mae: 1.2968\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.6409 - mae: 0.9615 - val_loss: 3.0737 - val_mae: 1.2959\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.6638 - mae: 0.9139 - val_loss: 3.0748 - val_mae: 1.3204\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.8251 - mae: 0.9709 - val_loss: 3.0843 - val_mae: 1.3307\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.8324 - mae: 0.9607 - val_loss: 3.0498 - val_mae: 1.3026\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.6961 - mae: 0.9218 - val_loss: 3.0559 - val_mae: 1.2867\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.6754 - mae: 0.9114 - val_loss: 3.1932 - val_mae: 1.2842\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.6093 - mae: 0.9239 - val_loss: 3.3188 - val_mae: 1.2823\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.6343 - mae: 0.9535 - val_loss: 3.2980 - val_mae: 1.2824\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.4767 - mae: 0.9181 - val_loss: 3.2213 - val_mae: 1.2865\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.5468 - mae: 0.9030 - val_loss: 3.1694 - val_mae: 1.2925\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.5400 - mae: 0.9065 - val_loss: 3.1352 - val_mae: 1.2969\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.4717 - mae: 0.9007 - val_loss: 3.1530 - val_mae: 1.2977\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.6192 - mae: 0.9391 - val_loss: 3.2076 - val_mae: 1.3005\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.6184 - mae: 0.9563 - val_loss: 3.1659 - val_mae: 1.3008\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.6143 - mae: 0.9525 - val_loss: 3.0771 - val_mae: 1.3000\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 1.6525 - mae: 0.9633 - val_loss: 3.0399 - val_mae: 1.2989\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.6772 - mae: 0.9461 - val_loss: 3.0469 - val_mae: 1.3022\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.6836 - mae: 0.9383 - val_loss: 3.0588 - val_mae: 1.2976\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.7061 - mae: 0.9463 - val_loss: 3.0758 - val_mae: 1.2973\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.6693 - mae: 0.9282 - val_loss: 3.0904 - val_mae: 1.2970\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.6644 - mae: 0.9242 - val_loss: 3.1137 - val_mae: 1.2961\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.6473 - mae: 0.9271 - val_loss: 3.1490 - val_mae: 1.2948\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.6395 - mae: 0.9336 - val_loss: 3.1734 - val_mae: 1.2944\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.5451 - mae: 0.8916 - val_loss: 3.1729 - val_mae: 1.2932\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.6196 - mae: 0.9211 - val_loss: 3.1554 - val_mae: 1.2916\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.6255 - mae: 0.9151 - val_loss: 3.1341 - val_mae: 1.2882\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 1.6253 - mae: 0.9080 - val_loss: 3.1247 - val_mae: 1.2855\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.4165 - mae: 0.8502 - val_loss: 3.1628 - val_mae: 1.2831\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.4427 - mae: 0.8828 - val_loss: 3.3267 - val_mae: 1.2841\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.3954 - mae: 0.9034 - val_loss: 3.6890 - val_mae: 1.3242\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.8521 - mae: 1.0851 - val_loss: 3.9880 - val_mae: 1.4033\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.9694 - mae: 1.1525 - val_loss: 3.7723 - val_mae: 1.3589\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.8896 - mae: 1.1059 - val_loss: 3.3385 - val_mae: 1.2953\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.5209 - mae: 0.9451 - val_loss: 3.1521 - val_mae: 1.2977\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.4466 - mae: 0.8805 - val_loss: 3.1063 - val_mae: 1.2994\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.6621 - mae: 0.9439 - val_loss: 3.0915 - val_mae: 1.3011\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.6735 - mae: 0.9524 - val_loss: 3.0951 - val_mae: 1.3023\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.6685 - mae: 0.9595 - val_loss: 3.1272 - val_mae: 1.3020\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.6157 - mae: 0.9521 - val_loss: 3.1544 - val_mae: 1.3016\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.5603 - mae: 0.9182 - val_loss: 3.1518 - val_mae: 1.3009\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.5479 - mae: 0.9066 - val_loss: 3.1647 - val_mae: 1.2998\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.6177 - mae: 0.9243 - val_loss: 3.1893 - val_mae: 1.2968\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.6030 - mae: 0.9263 - val_loss: 3.2253 - val_mae: 1.2957\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.5776 - mae: 0.9234 - val_loss: 3.2496 - val_mae: 1.2951\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.6132 - mae: 0.9401 - val_loss: 3.2070 - val_mae: 1.2952\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.5535 - mae: 0.8991 - val_loss: 3.1511 - val_mae: 1.2935\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.5592 - mae: 0.8854 - val_loss: 3.1065 - val_mae: 1.2920\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.6236 - mae: 0.8924 - val_loss: 3.1011 - val_mae: 1.3091\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.7288 - mae: 0.9344 - val_loss: 3.0918 - val_mae: 1.3018\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.7210 - mae: 0.9278 - val_loss: 3.0847 - val_mae: 1.2822\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.6499 - mae: 0.8875 - val_loss: 3.1008 - val_mae: 1.2783\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.5846 - mae: 0.8695 - val_loss: 3.1426 - val_mae: 1.2756\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.3742 - mae: 0.8461 - val_loss: 3.2903 - val_mae: 1.2738\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.6408 - mae: 0.9516 - val_loss: 3.5187 - val_mae: 1.2738\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.7627 - mae: 1.0364 - val_loss: 3.5743 - val_mae: 1.2847\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.6924 - mae: 1.0114 - val_loss: 3.3935 - val_mae: 1.2800\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.6421 - mae: 0.9638 - val_loss: 3.1937 - val_mae: 1.2865\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.5945 - mae: 0.9147 - val_loss: 3.1325 - val_mae: 1.2921\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.6164 - mae: 0.9079 - val_loss: 3.1275 - val_mae: 1.3120\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.6348 - mae: 0.9117 - val_loss: 3.1329 - val_mae: 1.3204\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.6994 - mae: 0.9332 - val_loss: 3.1313 - val_mae: 1.3140\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.5996 - mae: 0.8896 - val_loss: 3.1479 - val_mae: 1.2869\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.6110 - mae: 0.9020 - val_loss: 3.2793 - val_mae: 1.2817\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.5911 - mae: 0.9177 - val_loss: 3.3456 - val_mae: 1.2802\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.5959 - mae: 0.9305 - val_loss: 3.2693 - val_mae: 1.2818\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.3989 - mae: 0.8650 - val_loss: 3.2367 - val_mae: 1.2830\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.5544 - mae: 0.8918 - val_loss: 3.2507 - val_mae: 1.2845\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.5249 - mae: 0.8995 - val_loss: 3.1733 - val_mae: 1.2888\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.4224 - mae: 0.8774 - val_loss: 3.1483 - val_mae: 1.2926\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 3.5219 - mae: 1.2821\n",
            "Test Loss: 3.5218732357025146\n",
            "Test Mean Absolute Error: 1.2820662260055542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7804010a3520> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 53ms/step"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 10 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7804010a3520> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Training Mean Squared Error (ANN): 1.5451841021771018\n",
            "Validation Mean Squared Error (ANN): 3.1483331416365616\n",
            "Testing Mean Squared Error (ANN): 3.52187316494077\n",
            "Training Mean Absolute Error (ANN): 0.9013758826586934\n",
            "Validation Mean Absolute Error (ANN): 1.2926459431648256\n",
            "Testing Mean Absolute Error (ANN): 1.2820663213729857\n",
            "Training Mean Squared Error (SVM): 1.7559782439466878\n",
            "Validation Mean Squared Error (SVM): 3.1098024086275533\n",
            "Testing Mean Squared Error (SVM): 3.3109083736444997\n",
            "Training Mean Absolute Error (SVM): 1.0642497027640883\n",
            "Validation Mean Absolute Error (SVM): 1.3089101302367236\n",
            "Testing Mean Absolute Error (SVM): 1.3528309083567283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['VV_mean', 'VH_mean']]\n",
        "y = data['Plant_heig']\n",
        "\n",
        "# Splitting the data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# XGBoost\n",
        "# Convert to DMatrix format\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',  # For regression\n",
        "    'max_depth': 3,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 100,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(dtrain)\n",
        "valid_predictions = model.predict(dvalid)\n",
        "test_predictions = model.predict(dtest)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (XGBoost): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (XGBoost): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (XGBoost): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (XGBoost): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (XGBoost): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (XGBoost): {test_mae}')\n",
        "\n",
        "\n",
        "# Random Forest\n",
        "# Create and train the Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = rf_model.predict(X_train)\n",
        "valid_predictions = rf_model.predict(X_valid)\n",
        "test_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (Random Forest): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (Random Forest): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (Random Forest): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')\n",
        "\n",
        "\n",
        "# ANN\n",
        "# Create the ANN model\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Mean Absolute Error: {mae}')\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(X_train).flatten()\n",
        "valid_predictions = model.predict(X_valid).flatten()\n",
        "test_predictions = model.predict(X_test).flatten()\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (ANN): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (ANN): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (ANN): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (ANN): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (ANN): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (ANN): {test_mae}')\n",
        "\n",
        "\n",
        "# SVM\n",
        "# Create and train the SVM model\n",
        "svm_model = SVR(kernel='rbf')  # You can experiment with different kernels\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = svm_model.predict(X_train)\n",
        "valid_predictions = svm_model.predict(X_valid)\n",
        "test_predictions = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (SVM): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (SVM): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (SVM): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (SVM): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (SVM): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (SVM): {test_mae}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d1NpfHbQRYgL",
        "outputId": "b4c89322-5d73-45b6-a497-37feac8cb086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation-rmse:2.28612\n",
            "[1]\tvalidation-rmse:2.26851\n",
            "[2]\tvalidation-rmse:2.23701\n",
            "[3]\tvalidation-rmse:2.23113\n",
            "[4]\tvalidation-rmse:2.23733\n",
            "[5]\tvalidation-rmse:2.24740\n",
            "[6]\tvalidation-rmse:2.31115\n",
            "[7]\tvalidation-rmse:2.35258\n",
            "[8]\tvalidation-rmse:2.42076\n",
            "[9]\tvalidation-rmse:2.47334\n",
            "[10]\tvalidation-rmse:2.50627\n",
            "[11]\tvalidation-rmse:2.52546\n",
            "[12]\tvalidation-rmse:2.54399\n",
            "[13]\tvalidation-rmse:2.61133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [02:47:55] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"n_estimators\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Mean Squared Error (XGBoost): 4.1707470238104305\n",
            "Validation Mean Squared Error (XGBoost): 6.8190313455903455\n",
            "Testing Mean Squared Error (XGBoost): 10.920511148948629\n",
            "Training Mean Absolute Error (XGBoost): 1.536516174475352\n",
            "Validation Mean Absolute Error (XGBoost): 1.8543668540318807\n",
            "Testing Mean Absolute Error (XGBoost): 2.9124638795852658\n",
            "Training Mean Squared Error (Random Forest): 1.6840787883333326\n",
            "Validation Mean Squared Error (Random Forest): 6.061162555000016\n",
            "Testing Mean Squared Error (Random Forest): 13.119381085833338\n",
            "Training Mean Absolute Error (Random Forest): 0.9813888888888899\n",
            "Validation Mean Absolute Error (Random Forest): 2.0124333333333366\n",
            "Testing Mean Absolute Error (Random Forest): 3.1502416666666666\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 192ms/step - loss: 74.1863 - mae: 8.0501 - val_loss: 65.1176 - val_mae: 7.7459\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 59.7183 - mae: 7.1996 - val_loss: 51.0696 - val_mae: 6.7769\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 45.6410 - mae: 6.2401 - val_loss: 38.9684 - val_mae: 5.8139\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 37.1019 - mae: 5.5607 - val_loss: 29.1776 - val_mae: 4.8974\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 27.6019 - mae: 4.7183 - val_loss: 21.4034 - val_mae: 4.0229\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 21.1403 - mae: 4.0404 - val_loss: 15.4054 - val_mae: 3.2435\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 16.1757 - mae: 3.4600 - val_loss: 11.0451 - val_mae: 2.6937\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 13.1271 - mae: 3.0705 - val_loss: 8.2913 - val_mae: 2.2587\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 11.3930 - mae: 2.7797 - val_loss: 6.7120 - val_mae: 2.0495\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 10.2351 - mae: 2.6037 - val_loss: 5.8530 - val_mae: 1.9919\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 10.5708 - mae: 2.5815 - val_loss: 5.4818 - val_mae: 1.9931\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 10.5264 - mae: 2.4954 - val_loss: 5.3683 - val_mae: 1.9906\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 11.1609 - mae: 2.5167 - val_loss: 5.3812 - val_mae: 2.0062\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 10.6192 - mae: 2.4064 - val_loss: 5.4248 - val_mae: 2.0257\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 11.7097 - mae: 2.4967 - val_loss: 5.4402 - val_mae: 2.0313\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 11.4497 - mae: 2.4373 - val_loss: 5.4345 - val_mae: 2.0303\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 11.7261 - mae: 2.4862 - val_loss: 5.4065 - val_mae: 2.0211\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 11.3781 - mae: 2.4431 - val_loss: 5.3805 - val_mae: 2.0100\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 11.4438 - mae: 2.4926 - val_loss: 5.3635 - val_mae: 1.9991\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 11.2241 - mae: 2.4751 - val_loss: 5.3541 - val_mae: 1.9843\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 11.0290 - mae: 2.4799 - val_loss: 5.3803 - val_mae: 1.9826\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 11.0119 - mae: 2.5286 - val_loss: 5.4663 - val_mae: 1.9808\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 10.5233 - mae: 2.5038 - val_loss: 5.6128 - val_mae: 1.9789\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 10.4142 - mae: 2.5197 - val_loss: 5.7812 - val_mae: 1.9772\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 9.9390 - mae: 2.5105 - val_loss: 5.9622 - val_mae: 1.9758\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 10.6657 - mae: 2.6253 - val_loss: 6.1733 - val_mae: 1.9744\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 10.0891 - mae: 2.5461 - val_loss: 6.3497 - val_mae: 1.9887\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 10.6649 - mae: 2.6354 - val_loss: 6.4549 - val_mae: 2.0048\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 10.5001 - mae: 2.6253 - val_loss: 6.4519 - val_mae: 2.0042\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 10.2465 - mae: 2.5781 - val_loss: 6.3705 - val_mae: 1.9916\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 10.6510 - mae: 2.6366 - val_loss: 6.2823 - val_mae: 1.9788\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 10.3008 - mae: 2.5658 - val_loss: 6.1933 - val_mae: 1.9712\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 10.1252 - mae: 2.5455 - val_loss: 6.0934 - val_mae: 1.9706\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 9.4080 - mae: 2.4565 - val_loss: 5.9625 - val_mae: 1.9702\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 10.6215 - mae: 2.6083 - val_loss: 5.8262 - val_mae: 1.9700\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 10.3947 - mae: 2.5635 - val_loss: 5.6879 - val_mae: 1.9699\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 10.3284 - mae: 2.5228 - val_loss: 5.5484 - val_mae: 1.9699\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 10.4886 - mae: 2.5291 - val_loss: 5.4389 - val_mae: 1.9698\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 10.6629 - mae: 2.5112 - val_loss: 5.3719 - val_mae: 1.9696\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 10.7525 - mae: 2.4918 - val_loss: 5.3436 - val_mae: 1.9690\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 10.0724 - mae: 2.4222 - val_loss: 5.3618 - val_mae: 1.9677\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 10.6648 - mae: 2.4876 - val_loss: 5.4193 - val_mae: 1.9661\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 10.8023 - mae: 2.5525 - val_loss: 5.4691 - val_mae: 1.9648\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 9.9378 - mae: 2.4557 - val_loss: 5.5697 - val_mae: 1.9633\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 9.9939 - mae: 2.4526 - val_loss: 5.7578 - val_mae: 1.9614\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 10.5869 - mae: 2.5894 - val_loss: 5.9542 - val_mae: 1.9598\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 10.5757 - mae: 2.6008 - val_loss: 6.1155 - val_mae: 1.9584\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 10.1998 - mae: 2.5436 - val_loss: 6.2187 - val_mae: 1.9616\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 10.2267 - mae: 2.5439 - val_loss: 6.3925 - val_mae: 1.9884\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 10.5688 - mae: 2.6167 - val_loss: 6.6476 - val_mae: 2.0242\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 10.5436 - mae: 2.6241 - val_loss: 6.7392 - val_mae: 2.0360\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 10.1766 - mae: 2.5646 - val_loss: 6.6371 - val_mae: 2.0220\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 10.7775 - mae: 2.6677 - val_loss: 6.4360 - val_mae: 1.9934\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 10.4659 - mae: 2.6081 - val_loss: 6.2627 - val_mae: 1.9668\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 10.5855 - mae: 2.6191 - val_loss: 6.1421 - val_mae: 1.9516\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 10.6132 - mae: 2.6249 - val_loss: 6.0478 - val_mae: 1.9511\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 9.6493 - mae: 2.4854 - val_loss: 6.0628 - val_mae: 1.9503\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 10.5409 - mae: 2.6006 - val_loss: 6.1674 - val_mae: 1.9502\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 10.6175 - mae: 2.6212 - val_loss: 6.2304 - val_mae: 1.9602\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 10.3749 - mae: 2.5833 - val_loss: 6.2516 - val_mae: 1.9633\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 10.4154 - mae: 2.6060 - val_loss: 6.2024 - val_mae: 1.9553\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 10.6152 - mae: 2.6259 - val_loss: 6.0895 - val_mae: 1.9469\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 10.3504 - mae: 2.5686 - val_loss: 6.0174 - val_mae: 1.9465\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 9.9086 - mae: 2.5277 - val_loss: 6.0052 - val_mae: 1.9459\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 10.4484 - mae: 2.5754 - val_loss: 5.9470 - val_mae: 1.9456\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 9.2244 - mae: 2.4450 - val_loss: 5.9077 - val_mae: 1.9455\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 10.1259 - mae: 2.5265 - val_loss: 5.9641 - val_mae: 1.9451\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 10.4903 - mae: 2.5832 - val_loss: 5.9519 - val_mae: 1.9448\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 10.2353 - mae: 2.5417 - val_loss: 5.8254 - val_mae: 1.9451\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 10.2628 - mae: 2.5410 - val_loss: 5.6913 - val_mae: 1.9454\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 10.2825 - mae: 2.5190 - val_loss: 5.5917 - val_mae: 1.9453\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 9.6534 - mae: 2.4542 - val_loss: 5.5570 - val_mae: 1.9448\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 10.3865 - mae: 2.5276 - val_loss: 5.5928 - val_mae: 1.9439\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 10.2991 - mae: 2.4989 - val_loss: 5.5884 - val_mae: 1.9431\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 10.5509 - mae: 2.5658 - val_loss: 5.5872 - val_mae: 1.9422\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 10.4704 - mae: 2.5465 - val_loss: 5.5428 - val_mae: 1.9418\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 10.4668 - mae: 2.5323 - val_loss: 5.5060 - val_mae: 1.9414\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 10.3036 - mae: 2.5159 - val_loss: 5.5105 - val_mae: 1.9406\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 10.2357 - mae: 2.4950 - val_loss: 5.4588 - val_mae: 1.9404\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 10.2657 - mae: 2.4798 - val_loss: 5.3771 - val_mae: 1.9405\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 10.2488 - mae: 2.4669 - val_loss: 5.3224 - val_mae: 1.9407\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 10.5511 - mae: 2.4979 - val_loss: 5.3023 - val_mae: 1.9408\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 10.2773 - mae: 2.4424 - val_loss: 5.3066 - val_mae: 1.9407\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 10.1686 - mae: 2.4401 - val_loss: 5.3178 - val_mae: 1.9405\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 9.6354 - mae: 2.3944 - val_loss: 5.4035 - val_mae: 1.9393\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 10.3999 - mae: 2.5201 - val_loss: 5.5418 - val_mae: 1.9378\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 10.3029 - mae: 2.5122 - val_loss: 5.5693 - val_mae: 1.9371\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 10.1274 - mae: 2.4888 - val_loss: 5.4940 - val_mae: 1.9368\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 10.5023 - mae: 2.5396 - val_loss: 5.4108 - val_mae: 1.9363\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 10.3581 - mae: 2.5108 - val_loss: 5.3751 - val_mae: 1.9354\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 10.3414 - mae: 2.4890 - val_loss: 5.4025 - val_mae: 1.9336\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 9.8770 - mae: 2.4637 - val_loss: 5.5028 - val_mae: 1.9315\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 10.4237 - mae: 2.5176 - val_loss: 5.5848 - val_mae: 1.9299\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 10.3855 - mae: 2.5305 - val_loss: 5.6191 - val_mae: 1.9289\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 10.5208 - mae: 2.5662 - val_loss: 5.6466 - val_mae: 1.9280\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 9.5617 - mae: 2.4635 - val_loss: 5.6382 - val_mae: 1.9276\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 10.1844 - mae: 2.5175 - val_loss: 5.7004 - val_mae: 1.9270\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 10.2582 - mae: 2.5278 - val_loss: 5.7612 - val_mae: 1.9264\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 10.2147 - mae: 2.5264 - val_loss: 5.7353 - val_mae: 1.9261\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 10.4828 - mae: 2.5741 - val_loss: 5.6696 - val_mae: 1.9260\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 12.1681 - mae: 2.9964\n",
            "Test Loss: 12.16812515258789\n",
            "Test Mean Absolute Error: 2.996405839920044\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Training Mean Squared Error (ANN): 10.162578742282129\n",
            "Validation Mean Squared Error (ANN): 5.669600261182001\n",
            "Testing Mean Squared Error (ANN): 12.168124737256862\n",
            "Training Mean Absolute Error (ANN): 2.5206161864598595\n",
            "Validation Mean Absolute Error (ANN): 1.9260078859329222\n",
            "Testing Mean Absolute Error (ANN): 2.9964059591293335\n",
            "Training Mean Squared Error (SVM): 9.422518716679706\n",
            "Validation Mean Squared Error (SVM): 4.6976427129973235\n",
            "Testing Mean Squared Error (SVM): 8.843019716355851\n",
            "Training Mean Absolute Error (SVM): 2.202355692206774\n",
            "Validation Mean Absolute Error (SVM): 1.9332352688565477\n",
            "Testing Mean Absolute Error (SVM): 2.6504472704364215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['VH_mean', 'DBH_m_']]\n",
        "y = data['Plant_heig']\n",
        "\n",
        "# Splitting the data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# XGBoost\n",
        "# Convert to DMatrix format\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',  # For regression\n",
        "    'max_depth': 3,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 100,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(dtrain)\n",
        "valid_predictions = model.predict(dvalid)\n",
        "test_predictions = model.predict(dtest)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (XGBoost): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (XGBoost): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (XGBoost): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (XGBoost): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (XGBoost): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (XGBoost): {test_mae}')\n",
        "\n",
        "\n",
        "# Random Forest\n",
        "# Create and train the Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = rf_model.predict(X_train)\n",
        "valid_predictions = rf_model.predict(X_valid)\n",
        "test_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (Random Forest): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (Random Forest): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (Random Forest): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')\n",
        "\n",
        "\n",
        "# ANN\n",
        "# Create the ANN model\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Mean Absolute Error: {mae}')\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(X_train).flatten()\n",
        "valid_predictions = model.predict(X_valid).flatten()\n",
        "test_predictions = model.predict(X_test).flatten()\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (ANN): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (ANN): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (ANN): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (ANN): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (ANN): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (ANN): {test_mae}')\n",
        "\n",
        "\n",
        "# SVM\n",
        "# Create and train the SVM model\n",
        "svm_model = SVR(kernel='rbf')  # You can experiment with different kernels\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = svm_model.predict(X_train)\n",
        "valid_predictions = svm_model.predict(X_valid)\n",
        "test_predictions = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (SVM): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (SVM): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (SVM): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (SVM): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (SVM): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (SVM): {test_mae}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cbMdSrJNSwnO",
        "outputId": "e0637fa3-4a2c-41b0-a2e7-4c414e4c2030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation-rmse:2.27947\n",
            "[1]\tvalidation-rmse:2.26768\n",
            "[2]\tvalidation-rmse:2.22843\n",
            "[3]\tvalidation-rmse:2.23238\n",
            "[4]\tvalidation-rmse:2.22521\n",
            "[5]\tvalidation-rmse:2.21944\n",
            "[6]\tvalidation-rmse:2.24910\n",
            "[7]\tvalidation-rmse:2.26200\n",
            "[8]\tvalidation-rmse:2.27476\n",
            "[9]\tvalidation-rmse:2.29120\n",
            "[10]\tvalidation-rmse:2.29498\n",
            "[11]\tvalidation-rmse:2.29144\n",
            "[12]\tvalidation-rmse:2.28905\n",
            "[13]\tvalidation-rmse:2.28301\n",
            "[14]\tvalidation-rmse:2.28354\n",
            "[15]\tvalidation-rmse:2.30539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [02:53:16] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"n_estimators\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Mean Squared Error (XGBoost): 2.691337443918263\n",
            "Validation Mean Squared Error (XGBoost): 5.314822593031019\n",
            "Testing Mean Squared Error (XGBoost): 8.095863211929045\n",
            "Training Mean Absolute Error (XGBoost): 1.2721580539809332\n",
            "Validation Mean Absolute Error (XGBoost): 1.8578456870714826\n",
            "Testing Mean Absolute Error (XGBoost): 2.4032422224680583\n",
            "Training Mean Squared Error (Random Forest): 1.0943736091666625\n",
            "Validation Mean Squared Error (Random Forest): 7.293126125000016\n",
            "Testing Mean Squared Error (Random Forest): 9.238662606666672\n",
            "Training Mean Absolute Error (Random Forest): 0.835324999999999\n",
            "Validation Mean Absolute Error (Random Forest): 2.0792000000000006\n",
            "Testing Mean Absolute Error (Random Forest): 2.6335\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 299ms/step - loss: 115.7008 - mae: 10.2907 - val_loss: 99.9691 - val_mae: 9.7494\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 94.2174 - mae: 9.2452 - val_loss: 79.7861 - val_mae: 8.6606\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 74.5784 - mae: 8.0914 - val_loss: 63.2824 - val_mae: 7.6536\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 58.8028 - mae: 7.1220 - val_loss: 49.2581 - val_mae: 6.6790\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 46.7619 - mae: 6.3459 - val_loss: 37.5961 - val_mae: 5.7451\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 35.5469 - mae: 5.4425 - val_loss: 28.5409 - val_mae: 4.8983\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 27.9317 - mae: 4.7680 - val_loss: 21.6353 - val_mae: 4.1375\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 21.6284 - mae: 4.1140 - val_loss: 16.3031 - val_mae: 3.4636\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 16.9734 - mae: 3.5587 - val_loss: 12.3289 - val_mae: 2.9506\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 13.5084 - mae: 3.1140 - val_loss: 9.3575 - val_mae: 2.4932\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 10.8414 - mae: 2.7158 - val_loss: 7.2044 - val_mae: 2.1225\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 9.5961 - mae: 2.4990 - val_loss: 5.7548 - val_mae: 1.8676\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 8.4945 - mae: 2.3102 - val_loss: 4.9002 - val_mae: 1.7464\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 8.2907 - mae: 2.2177 - val_loss: 4.5115 - val_mae: 1.7021\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.7952 - mae: 2.0941 - val_loss: 4.4558 - val_mae: 1.7021\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 8.6271 - mae: 2.1784 - val_loss: 4.5660 - val_mae: 1.7255\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 9.0321 - mae: 2.2342 - val_loss: 4.7445 - val_mae: 1.7543\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 9.2604 - mae: 2.2774 - val_loss: 4.9097 - val_mae: 1.7724\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 8.9614 - mae: 2.2618 - val_loss: 4.9468 - val_mae: 1.7734\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 9.0386 - mae: 2.2935 - val_loss: 4.8511 - val_mae: 1.7586\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 8.7882 - mae: 2.2474 - val_loss: 4.6977 - val_mae: 1.7336\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 9.0101 - mae: 2.2448 - val_loss: 4.5547 - val_mae: 1.7027\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 8.2884 - mae: 2.1847 - val_loss: 4.4646 - val_mae: 1.6980\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 8.6231 - mae: 2.2036 - val_loss: 4.4473 - val_mae: 1.6976\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 8.1213 - mae: 2.1155 - val_loss: 4.4870 - val_mae: 1.6960\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 8.0646 - mae: 2.1252 - val_loss: 4.5566 - val_mae: 1.6950\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 7.8060 - mae: 2.0972 - val_loss: 4.6231 - val_mae: 1.7020\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 8.0858 - mae: 2.1721 - val_loss: 4.6714 - val_mae: 1.7103\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 7.5039 - mae: 2.0956 - val_loss: 4.7264 - val_mae: 1.7184\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 7.9627 - mae: 2.1509 - val_loss: 4.7893 - val_mae: 1.7268\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 7.4422 - mae: 2.0893 - val_loss: 4.8618 - val_mae: 1.7354\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 7.7791 - mae: 2.1299 - val_loss: 4.9396 - val_mae: 1.7436\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 7.3734 - mae: 2.0972 - val_loss: 5.0170 - val_mae: 1.7513\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 7.5381 - mae: 2.1453 - val_loss: 5.1149 - val_mae: 1.7608\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 7.8216 - mae: 2.1686 - val_loss: 5.1821 - val_mae: 1.7668\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 7.5925 - mae: 2.1418 - val_loss: 5.1771 - val_mae: 1.7652\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 7.8384 - mae: 2.1714 - val_loss: 5.1512 - val_mae: 1.7614\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 7.8704 - mae: 2.1850 - val_loss: 5.1407 - val_mae: 1.7591\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 7.4207 - mae: 2.1012 - val_loss: 5.0957 - val_mae: 1.7532\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 7.7028 - mae: 2.1425 - val_loss: 5.0066 - val_mae: 1.7423\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 7.6442 - mae: 2.1262 - val_loss: 4.9206 - val_mae: 1.7308\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 7.2966 - mae: 2.0651 - val_loss: 4.8126 - val_mae: 1.7148\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 7.5553 - mae: 2.0878 - val_loss: 4.7079 - val_mae: 1.6964\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 7.2113 - mae: 1.9984 - val_loss: 4.6476 - val_mae: 1.6830\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0613 - mae: 1.9865 - val_loss: 4.6401 - val_mae: 1.6794\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.4966 - mae: 2.0556 - val_loss: 4.6764 - val_mae: 1.6850\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 7.3612 - mae: 2.0298 - val_loss: 4.7234 - val_mae: 1.6920\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 6.7740 - mae: 1.9591 - val_loss: 4.7813 - val_mae: 1.7001\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 6.9424 - mae: 1.9595 - val_loss: 4.8196 - val_mae: 1.7039\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.0586 - mae: 2.0033 - val_loss: 4.7849 - val_mae: 1.6954\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.3496 - mae: 2.0465 - val_loss: 4.7298 - val_mae: 1.6827\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.2944 - mae: 2.0183 - val_loss: 4.6950 - val_mae: 1.6726\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 7.2916 - mae: 2.0133 - val_loss: 4.6680 - val_mae: 1.6630\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.8975 - mae: 1.9433 - val_loss: 4.6355 - val_mae: 1.6561\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.2551 - mae: 2.0152 - val_loss: 4.6047 - val_mae: 1.6548\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 6.9073 - mae: 1.9324 - val_loss: 4.5966 - val_mae: 1.6537\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 7.3580 - mae: 2.0316 - val_loss: 4.6134 - val_mae: 1.6528\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.3633 - mae: 2.0385 - val_loss: 4.6323 - val_mae: 1.6520\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 7.1687 - mae: 1.9887 - val_loss: 4.6426 - val_mae: 1.6511\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 6.9106 - mae: 1.9808 - val_loss: 4.6460 - val_mae: 1.6498\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 6.8100 - mae: 1.9585 - val_loss: 4.6607 - val_mae: 1.6481\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 6.8056 - mae: 1.9324 - val_loss: 4.7012 - val_mae: 1.6467\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.8812 - mae: 1.9518 - val_loss: 4.7497 - val_mae: 1.6458\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7808 - mae: 1.9154 - val_loss: 4.7932 - val_mae: 1.6557\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 7.0713 - mae: 1.9943 - val_loss: 4.8467 - val_mae: 1.6667\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9971 - mae: 1.9725 - val_loss: 4.9066 - val_mae: 1.6773\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 6.9637 - mae: 1.9758 - val_loss: 4.9829 - val_mae: 1.6895\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 6.6061 - mae: 1.9223 - val_loss: 5.1059 - val_mae: 1.7070\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 6.9912 - mae: 1.9982 - val_loss: 5.2619 - val_mae: 1.7257\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.2258 - mae: 1.8977 - val_loss: 5.3520 - val_mae: 1.7410\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 7.0956 - mae: 2.0255 - val_loss: 5.3331 - val_mae: 1.7357\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 6.8153 - mae: 1.9653 - val_loss: 5.2075 - val_mae: 1.7143\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 6.9803 - mae: 2.0029 - val_loss: 5.0525 - val_mae: 1.6898\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 6.7783 - mae: 1.9409 - val_loss: 4.9865 - val_mae: 1.6759\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 6.8931 - mae: 1.9777 - val_loss: 4.9790 - val_mae: 1.6722\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7803 - mae: 1.9481 - val_loss: 4.9608 - val_mae: 1.6665\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7440 - mae: 1.9381 - val_loss: 4.9198 - val_mae: 1.6550\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.7173 - mae: 1.9248 - val_loss: 4.8818 - val_mae: 1.6422\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4963 - mae: 1.8837 - val_loss: 4.8495 - val_mae: 1.6340\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.5014 - mae: 1.9109 - val_loss: 4.8378 - val_mae: 1.6336\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.3753 - mae: 1.8721 - val_loss: 4.8364 - val_mae: 1.6331\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.6126 - mae: 1.9116 - val_loss: 4.8397 - val_mae: 1.6322\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7788 - mae: 1.9640 - val_loss: 4.8540 - val_mae: 1.6310\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9124 - mae: 1.9851 - val_loss: 4.8736 - val_mae: 1.6300\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 6.3950 - mae: 1.9250 - val_loss: 4.8927 - val_mae: 1.6339\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 6.7626 - mae: 1.9606 - val_loss: 4.9017 - val_mae: 1.6294\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 6.6671 - mae: 1.9519 - val_loss: 4.9120 - val_mae: 1.6278\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7189 - mae: 1.9403 - val_loss: 4.9215 - val_mae: 1.6275\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.2139 - mae: 1.8570 - val_loss: 4.9523 - val_mae: 1.6275\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 6.7691 - mae: 1.9584 - val_loss: 5.0559 - val_mae: 1.6549\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.7718 - mae: 1.9635 - val_loss: 5.2064 - val_mae: 1.6836\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.8116 - mae: 1.9774 - val_loss: 5.3501 - val_mae: 1.7095\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.5083 - mae: 1.9178 - val_loss: 5.4816 - val_mae: 1.7409\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 6.5145 - mae: 1.9183 - val_loss: 5.5790 - val_mae: 1.7615\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9835 - mae: 2.0201 - val_loss: 5.5980 - val_mae: 1.7646\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.5887 - mae: 1.9242 - val_loss: 5.5396 - val_mae: 1.7506\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 6.5336 - mae: 1.9419 - val_loss: 5.3981 - val_mae: 1.7149\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 6.7708 - mae: 1.9522 - val_loss: 5.2610 - val_mae: 1.6817\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 6.7114 - mae: 1.9457 - val_loss: 5.1562 - val_mae: 1.6602\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.6102 - mae: 1.9147 - val_loss: 5.0753 - val_mae: 1.6373\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 9.9552 - mae: 2.6358\n",
            "Test Loss: 9.955159187316895\n",
            "Test Mean Absolute Error: 2.6357738971710205\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Training Mean Squared Error (ANN): 6.421008974964111\n",
            "Validation Mean Squared Error (ANN): 5.07526168537402\n",
            "Testing Mean Squared Error (ANN): 9.955159777793751\n",
            "Training Mean Absolute Error (ANN): 1.8949565977520413\n",
            "Validation Mean Absolute Error (ANN): 1.63729584534963\n",
            "Testing Mean Absolute Error (ANN): 2.6357741355896\n",
            "Training Mean Squared Error (SVM): 8.360901167635522\n",
            "Validation Mean Squared Error (SVM): 4.581230377623073\n",
            "Testing Mean Squared Error (SVM): 9.967230663831977\n",
            "Training Mean Absolute Error (SVM): 2.134421157204755\n",
            "Validation Mean Absolute Error (SVM): 1.77557981918273\n",
            "Testing Mean Absolute Error (SVM): 2.7495326482399993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['VV_mean', 'DBH_m_']]\n",
        "y = data['Plant_heig']\n",
        "\n",
        "# Splitting the data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# XGBoost\n",
        "# Convert to DMatrix format\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',  # For regression\n",
        "    'max_depth': 3,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 100,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(dtrain)\n",
        "valid_predictions = model.predict(dvalid)\n",
        "test_predictions = model.predict(dtest)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (XGBoost): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (XGBoost): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (XGBoost): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (XGBoost): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (XGBoost): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (XGBoost): {test_mae}')\n",
        "\n",
        "\n",
        "# Random Forest\n",
        "# Create and train the Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = rf_model.predict(X_train)\n",
        "valid_predictions = rf_model.predict(X_valid)\n",
        "test_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (Random Forest): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (Random Forest): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (Random Forest): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')\n",
        "\n",
        "\n",
        "# ANN\n",
        "# Create the ANN model\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Mean Absolute Error: {mae}')\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(X_train).flatten()\n",
        "valid_predictions = model.predict(X_valid).flatten()\n",
        "test_predictions = model.predict(X_test).flatten()\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (ANN): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (ANN): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (ANN): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (ANN): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (ANN): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (ANN): {test_mae}')\n",
        "\n",
        "\n",
        "# SVM\n",
        "# Create and train the SVM model\n",
        "svm_model = SVR(kernel='rbf')  # You can experiment with different kernels\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = svm_model.predict(X_train)\n",
        "valid_predictions = svm_model.predict(X_valid)\n",
        "test_predictions = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (SVM): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (SVM): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (SVM): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (SVM): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (SVM): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (SVM): {test_mae}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ANwSws4fTEF9",
        "outputId": "2965b446-15f8-436a-8f0e-7eae9d447eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation-rmse:2.25253\n",
            "[1]\tvalidation-rmse:2.21893\n",
            "[2]\tvalidation-rmse:2.22657\n",
            "[3]\tvalidation-rmse:2.21834\n",
            "[4]\tvalidation-rmse:2.22134\n",
            "[5]\tvalidation-rmse:2.23378\n",
            "[6]\tvalidation-rmse:2.25260\n",
            "[7]\tvalidation-rmse:2.28676\n",
            "[8]\tvalidation-rmse:2.32876\n",
            "[9]\tvalidation-rmse:2.36211\n",
            "[10]\tvalidation-rmse:2.38712\n",
            "[11]\tvalidation-rmse:2.39876\n",
            "[12]\tvalidation-rmse:2.44486\n",
            "[13]\tvalidation-rmse:2.48185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [02:54:18] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"n_estimators\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Mean Squared Error (XGBoost): 2.604469304360516\n",
            "Validation Mean Squared Error (XGBoost): 6.159583307179854\n",
            "Testing Mean Squared Error (XGBoost): 8.184111508029547\n",
            "Training Mean Absolute Error (XGBoost): 1.236486632823944\n",
            "Validation Mean Absolute Error (XGBoost): 1.9493010234832768\n",
            "Testing Mean Absolute Error (XGBoost): 2.520942791302999\n",
            "Training Mean Squared Error (Random Forest): 1.0958011080555528\n",
            "Validation Mean Squared Error (Random Forest): 5.501696700000014\n",
            "Testing Mean Squared Error (Random Forest): 7.854126785833351\n",
            "Training Mean Absolute Error (Random Forest): 0.8133861111111109\n",
            "Validation Mean Absolute Error (Random Forest): 1.992583333333335\n",
            "Testing Mean Absolute Error (Random Forest): 2.1903416666666673\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 343ms/step - loss: 43.5541 - mae: 6.0921 - val_loss: 41.4461 - val_mae: 6.0390\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 39.1236 - mae: 5.7356 - val_loss: 36.8442 - val_mae: 5.6437\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 34.6181 - mae: 5.3305 - val_loss: 32.5332 - val_mae: 5.2447\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 30.0341 - mae: 4.9569 - val_loss: 28.5259 - val_mae: 4.8424\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 27.3425 - mae: 4.6825 - val_loss: 24.8544 - val_mae: 4.4407\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 23.8408 - mae: 4.3385 - val_loss: 21.5470 - val_mae: 4.1134\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 21.0531 - mae: 4.0548 - val_loss: 18.5921 - val_mae: 3.8097\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 18.1355 - mae: 3.7154 - val_loss: 16.0363 - val_mae: 3.5163\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 16.1888 - mae: 3.4664 - val_loss: 13.8771 - val_mae: 3.2368\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 13.7434 - mae: 3.1099 - val_loss: 12.0789 - val_mae: 2.9706\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 12.1544 - mae: 2.8894 - val_loss: 10.5863 - val_mae: 2.7162\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 10.5831 - mae: 2.6758 - val_loss: 9.3374 - val_mae: 2.5120\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 9.9712 - mae: 2.6099 - val_loss: 8.3138 - val_mae: 2.3524\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 8.8344 - mae: 2.4338 - val_loss: 7.5248 - val_mae: 2.2148\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 8.3228 - mae: 2.3661 - val_loss: 6.9447 - val_mae: 2.0991\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 8.1774 - mae: 2.2975 - val_loss: 6.5509 - val_mae: 2.0611\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 7.9508 - mae: 2.2284 - val_loss: 6.3258 - val_mae: 2.0263\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 7.8238 - mae: 2.1687 - val_loss: 6.2348 - val_mae: 1.9948\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 7.8196 - mae: 2.1452 - val_loss: 6.2449 - val_mae: 1.9659\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 8.0854 - mae: 2.2035 - val_loss: 6.3213 - val_mae: 1.9420\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 7.7960 - mae: 2.1672 - val_loss: 6.4082 - val_mae: 1.9256\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 8.2877 - mae: 2.2185 - val_loss: 6.4648 - val_mae: 1.9170\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 8.3555 - mae: 2.2475 - val_loss: 6.4918 - val_mae: 1.9131\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 8.3461 - mae: 2.2271 - val_loss: 6.5068 - val_mae: 1.9109\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 8.2412 - mae: 2.2088 - val_loss: 6.5158 - val_mae: 1.9092\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 8.2866 - mae: 2.2276 - val_loss: 6.5174 - val_mae: 1.9085\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 7.3052 - mae: 2.1006 - val_loss: 6.4893 - val_mae: 1.9118\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 8.2402 - mae: 2.2243 - val_loss: 6.4379 - val_mae: 1.9188\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 7.6739 - mae: 2.1524 - val_loss: 6.3847 - val_mae: 1.9278\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 7.7870 - mae: 2.1437 - val_loss: 6.3388 - val_mae: 1.9381\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 7.6726 - mae: 2.1282 - val_loss: 6.3050 - val_mae: 1.9504\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 7.4553 - mae: 2.0832 - val_loss: 6.2935 - val_mae: 1.9637\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 7.5699 - mae: 2.1131 - val_loss: 6.3040 - val_mae: 1.9752\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 7.1106 - mae: 2.0343 - val_loss: 6.3394 - val_mae: 1.9887\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 7.3523 - mae: 2.0928 - val_loss: 6.3970 - val_mae: 2.0012\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 7.2509 - mae: 2.0932 - val_loss: 6.4335 - val_mae: 2.0058\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 7.5195 - mae: 2.1267 - val_loss: 6.4520 - val_mae: 2.0065\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 7.5082 - mae: 2.1323 - val_loss: 6.4648 - val_mae: 2.0060\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 7.1537 - mae: 2.0797 - val_loss: 6.4802 - val_mae: 2.0058\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 7.1201 - mae: 2.0498 - val_loss: 6.4888 - val_mae: 2.0043\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 7.3389 - mae: 2.0967 - val_loss: 6.4686 - val_mae: 1.9977\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 7.4046 - mae: 2.1177 - val_loss: 6.4404 - val_mae: 1.9884\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 7.3054 - mae: 2.0858 - val_loss: 6.4209 - val_mae: 1.9796\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 7.1645 - mae: 2.0617 - val_loss: 6.4130 - val_mae: 1.9724\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 7.2319 - mae: 2.0860 - val_loss: 6.4107 - val_mae: 1.9646\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 7.2960 - mae: 2.0968 - val_loss: 6.4165 - val_mae: 1.9588\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 7.0323 - mae: 2.0542 - val_loss: 6.4245 - val_mae: 1.9531\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 7.1276 - mae: 2.0562 - val_loss: 6.4357 - val_mae: 1.9426\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 7.3093 - mae: 2.1059 - val_loss: 6.4567 - val_mae: 1.9313\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 6.9244 - mae: 2.0318 - val_loss: 6.4753 - val_mae: 1.9253\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 7.2933 - mae: 2.0940 - val_loss: 6.4855 - val_mae: 1.9241\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 6.8589 - mae: 2.0242 - val_loss: 6.4882 - val_mae: 1.9264\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 7.3036 - mae: 2.1153 - val_loss: 6.4872 - val_mae: 1.9329\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 7.0854 - mae: 2.0694 - val_loss: 6.4933 - val_mae: 1.9373\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 7.0679 - mae: 2.0638 - val_loss: 6.5032 - val_mae: 1.9412\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 7.1282 - mae: 2.0740 - val_loss: 6.5167 - val_mae: 1.9463\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 7.0184 - mae: 2.0582 - val_loss: 6.5319 - val_mae: 1.9492\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.9713 - mae: 2.0621 - val_loss: 6.5499 - val_mae: 1.9518\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.7432 - mae: 2.0031 - val_loss: 6.5684 - val_mae: 1.9540\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 6.5050 - mae: 1.9706 - val_loss: 6.5843 - val_mae: 1.9543\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 6.9249 - mae: 2.0519 - val_loss: 6.6044 - val_mae: 1.9555\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 6.8945 - mae: 2.0329 - val_loss: 6.6329 - val_mae: 1.9588\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 6.6403 - mae: 1.9964 - val_loss: 6.6788 - val_mae: 1.9655\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.6415 - mae: 2.0082 - val_loss: 6.7300 - val_mae: 1.9722\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 6.3776 - mae: 1.9694 - val_loss: 6.7646 - val_mae: 1.9752\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 7.0432 - mae: 2.0790 - val_loss: 6.7835 - val_mae: 1.9751\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 6.9296 - mae: 2.0471 - val_loss: 6.7949 - val_mae: 1.9735\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 7.0382 - mae: 2.0879 - val_loss: 6.8072 - val_mae: 1.9718\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 6.3054 - mae: 1.9365 - val_loss: 6.7907 - val_mae: 1.9652\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 6.7861 - mae: 2.0209 - val_loss: 6.7610 - val_mae: 1.9549\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 6.4503 - mae: 1.9794 - val_loss: 6.7468 - val_mae: 1.9446\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 6.6363 - mae: 1.9961 - val_loss: 6.7430 - val_mae: 1.9341\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 6.6179 - mae: 1.9916 - val_loss: 6.7463 - val_mae: 1.9240\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 6.4953 - mae: 1.9757 - val_loss: 6.7593 - val_mae: 1.9279\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 6.6857 - mae: 1.9899 - val_loss: 6.7774 - val_mae: 1.9318\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 6.4755 - mae: 1.9488 - val_loss: 6.7935 - val_mae: 1.9324\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 6.3479 - mae: 1.9396 - val_loss: 6.8126 - val_mae: 1.9292\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 6.2554 - mae: 1.9431 - val_loss: 6.8352 - val_mae: 1.9273\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 6.4044 - mae: 1.9596 - val_loss: 6.8528 - val_mae: 1.9288\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 6.5098 - mae: 1.9624 - val_loss: 6.8737 - val_mae: 1.9299\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 6.4769 - mae: 1.9488 - val_loss: 6.8968 - val_mae: 1.9305\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 6.6507 - mae: 2.0037 - val_loss: 6.9180 - val_mae: 1.9323\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 6.2702 - mae: 1.9467 - val_loss: 6.9329 - val_mae: 1.9353\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.2867 - mae: 1.9294 - val_loss: 6.9436 - val_mae: 1.9382\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.4960 - mae: 1.9689 - val_loss: 6.9526 - val_mae: 1.9402\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5526 - mae: 1.9834 - val_loss: 6.9620 - val_mae: 1.9413\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.1683 - mae: 1.9135 - val_loss: 6.9734 - val_mae: 1.9411\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 6.3626 - mae: 1.9593 - val_loss: 6.9905 - val_mae: 1.9402\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 6.4463 - mae: 1.9614 - val_loss: 7.0177 - val_mae: 1.9380\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.5482 - mae: 1.9794 - val_loss: 7.0488 - val_mae: 1.9444\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 6.0819 - mae: 1.9038 - val_loss: 7.0711 - val_mae: 1.9484\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 6.2142 - mae: 1.9187 - val_loss: 7.0878 - val_mae: 1.9514\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.4376 - mae: 1.9573 - val_loss: 7.1029 - val_mae: 1.9539\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.3518 - mae: 1.9464 - val_loss: 7.1267 - val_mae: 1.9577\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.1333 - mae: 1.8924 - val_loss: 7.1487 - val_mae: 1.9612\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 6.5409 - mae: 1.9813 - val_loss: 7.1580 - val_mae: 1.9618\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 6.5353 - mae: 1.9785 - val_loss: 7.1682 - val_mae: 1.9621\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 6.1308 - mae: 1.8852 - val_loss: 7.1793 - val_mae: 1.9628\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 6.1252 - mae: 1.8849 - val_loss: 7.1871 - val_mae: 1.9622\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 6.4093 - mae: 1.9508 - val_loss: 7.1949 - val_mae: 1.9611\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 11.4679 - mae: 2.7126\n",
            "Test Loss: 11.467913627624512\n",
            "Test Mean Absolute Error: 2.712601661682129\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Training Mean Squared Error (ANN): 6.246930936906072\n",
            "Validation Mean Squared Error (ANN): 7.19494736195097\n",
            "Testing Mean Squared Error (ANN): 11.467913611484947\n",
            "Training Mean Absolute Error (ANN): 1.918233575820923\n",
            "Validation Mean Absolute Error (ANN): 1.961079497337341\n",
            "Testing Mean Absolute Error (ANN): 2.7126018285751345\n",
            "Training Mean Squared Error (SVM): 7.359556446647158\n",
            "Validation Mean Squared Error (SVM): 4.2515591064263445\n",
            "Testing Mean Squared Error (SVM): 8.790626226092408\n",
            "Training Mean Absolute Error (SVM): 1.9708732284570856\n",
            "Validation Mean Absolute Error (SVM): 1.6529726834965348\n",
            "Testing Mean Absolute Error (SVM): 2.553883745199727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['VV_mean', 'VH_mean']]\n",
        "y = data['canopy_cir']\n",
        "\n",
        "# Splitting the data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# XGBoost\n",
        "# Convert to DMatrix format\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',  # For regression\n",
        "    'max_depth': 3,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 100,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(dtrain)\n",
        "valid_predictions = model.predict(dvalid)\n",
        "test_predictions = model.predict(dtest)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (XGBoost): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (XGBoost): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (XGBoost): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (XGBoost): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (XGBoost): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (XGBoost): {test_mae}')\n",
        "\n",
        "\n",
        "# Random Forest\n",
        "# Create and train the Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = rf_model.predict(X_train)\n",
        "valid_predictions = rf_model.predict(X_valid)\n",
        "test_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (Random Forest): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (Random Forest): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (Random Forest): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')\n",
        "\n",
        "\n",
        "# ANN\n",
        "# Create the ANN model\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Mean Absolute Error: {mae}')\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(X_train).flatten()\n",
        "valid_predictions = model.predict(X_valid).flatten()\n",
        "test_predictions = model.predict(X_test).flatten()\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (ANN): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (ANN): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (ANN): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (ANN): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (ANN): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (ANN): {test_mae}')\n",
        "\n",
        "\n",
        "# SVM\n",
        "# Create and train the SVM model\n",
        "svm_model = SVR(kernel='rbf')  # You can experiment with different kernels\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = svm_model.predict(X_train)\n",
        "valid_predictions = svm_model.predict(X_valid)\n",
        "test_predictions = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (SVM): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (SVM): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (SVM): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (SVM): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (SVM): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (SVM): {test_mae}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtQULKy0TrPW",
        "outputId": "ac8dc29e-d3e5-43fc-e10b-a989bb41d881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation-rmse:10.34023\n",
            "[1]\tvalidation-rmse:10.78607\n",
            "[2]\tvalidation-rmse:11.23754\n",
            "[3]\tvalidation-rmse:11.67790\n",
            "[4]\tvalidation-rmse:12.20845\n",
            "[5]\tvalidation-rmse:12.70214\n",
            "[6]\tvalidation-rmse:13.19290\n",
            "[7]\tvalidation-rmse:13.75272\n",
            "[8]\tvalidation-rmse:14.19795\n",
            "[9]\tvalidation-rmse:14.62929\n",
            "[10]\tvalidation-rmse:15.03749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [02:57:17] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"n_estimators\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Mean Squared Error (XGBoost): 113.06926980915402\n",
            "Validation Mean Squared Error (XGBoost): 226.1260338146652\n",
            "Testing Mean Squared Error (XGBoost): 229.6568931284834\n",
            "Training Mean Absolute Error (XGBoost): 8.104272884792751\n",
            "Validation Mean Absolute Error (XGBoost): 12.318693161010742\n",
            "Testing Mean Absolute Error (XGBoost): 9.892454401652019\n",
            "Training Mean Squared Error (Random Forest): 34.60345066666666\n",
            "Validation Mean Squared Error (Random Forest): 438.14272983333325\n",
            "Testing Mean Squared Error (Random Forest): 403.54010450000004\n",
            "Training Mean Absolute Error (Random Forest): 4.687111111111111\n",
            "Validation Mean Absolute Error (Random Forest): 16.39933333333333\n",
            "Testing Mean Absolute Error (Random Forest): 16.519166666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 351ms/step - loss: 1915.9155 - mae: 41.1521 - val_loss: 1587.7583 - val_mae: 38.6666\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 1867.7819 - mae: 40.5966 - val_loss: 1549.6221 - val_mae: 38.1717\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 1871.7924 - mae: 40.4616 - val_loss: 1511.9054 - val_mae: 37.6759\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1841.0981 - mae: 40.1819 - val_loss: 1474.7725 - val_mae: 37.1816\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1788.8507 - mae: 39.6439 - val_loss: 1437.7721 - val_mae: 36.6824\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 1733.3142 - mae: 38.7714 - val_loss: 1400.5375 - val_mae: 36.1730\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1696.0369 - mae: 38.3744 - val_loss: 1362.9324 - val_mae: 35.6512\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 1707.5552 - mae: 38.3919 - val_loss: 1325.3779 - val_mae: 35.1222\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 1653.3573 - mae: 37.6297 - val_loss: 1287.8470 - val_mae: 34.5854\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 1572.1885 - mae: 36.7091 - val_loss: 1249.7711 - val_mae: 34.0322\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1521.5997 - mae: 35.9241 - val_loss: 1210.8331 - val_mae: 33.4571\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 1511.2853 - mae: 35.7416 - val_loss: 1171.2325 - val_mae: 32.8619\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 1485.2242 - mae: 35.3106 - val_loss: 1131.4506 - val_mae: 32.2529\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1441.4033 - mae: 34.6835 - val_loss: 1091.6412 - val_mae: 31.6318\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1376.8844 - mae: 33.7879 - val_loss: 1051.5302 - val_mae: 30.9934\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1365.4028 - mae: 33.6361 - val_loss: 1011.3017 - val_mae: 30.3396\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 1262.0721 - mae: 32.3074 - val_loss: 968.8958 - val_mae: 29.6350\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1257.1849 - mae: 31.9881 - val_loss: 926.2770 - val_mae: 28.9094\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1217.4290 - mae: 31.2782 - val_loss: 883.1651 - val_mae: 28.1564\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 1116.0101 - mae: 29.8092 - val_loss: 839.0877 - val_mae: 27.3652\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1140.8549 - mae: 30.2076 - val_loss: 795.0815 - val_mae: 26.5517\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1042.5483 - mae: 28.4455 - val_loss: 748.9709 - val_mae: 25.6724\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1001.8741 - mae: 28.1417 - val_loss: 700.9075 - val_mae: 24.7223\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 947.2824 - mae: 26.9605 - val_loss: 652.5709 - val_mae: 23.7285\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 909.5609 - mae: 26.1793 - val_loss: 604.3184 - val_mae: 22.6932\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 856.5034 - mae: 25.2584 - val_loss: 556.9048 - val_mae: 21.6277\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 812.1293 - mae: 24.3597 - val_loss: 510.7877 - val_mae: 20.5386\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 735.4053 - mae: 23.0130 - val_loss: 466.3489 - val_mae: 19.4322\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 660.0998 - mae: 21.6356 - val_loss: 424.2284 - val_mae: 18.3211\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 659.7287 - mae: 21.3856 - val_loss: 383.5006 - val_mae: 17.1792\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 590.8476 - mae: 20.0783 - val_loss: 344.8387 - val_mae: 16.0203\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 547.1403 - mae: 19.4378 - val_loss: 308.6045 - val_mae: 14.8526\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 491.8863 - mae: 18.1218 - val_loss: 275.2016 - val_mae: 13.6885\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 492.1784 - mae: 17.8116 - val_loss: 244.9781 - val_mae: 12.5428\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 441.1600 - mae: 16.5718 - val_loss: 217.9728 - val_mae: 11.4228\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 424.2591 - mae: 16.0898 - val_loss: 193.5902 - val_mae: 10.4065\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 371.6791 - mae: 14.9791 - val_loss: 172.0026 - val_mae: 9.4923\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 356.3593 - mae: 14.1684 - val_loss: 152.7684 - val_mae: 8.5852\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 332.6556 - mae: 13.4599 - val_loss: 136.2629 - val_mae: 8.0200\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - loss: 309.6483 - mae: 12.8629 - val_loss: 122.7131 - val_mae: 7.4983\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 289.1727 - mae: 12.3938 - val_loss: 111.6242 - val_mae: 7.0422\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - loss: 290.7134 - mae: 12.5901 - val_loss: 102.8050 - val_mae: 7.1040\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - loss: 274.7943 - mae: 12.1754 - val_loss: 96.4303 - val_mae: 7.2383\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 269.9857 - mae: 12.1893 - val_loss: 92.1225 - val_mae: 7.3551\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 251.2857 - mae: 11.6085 - val_loss: 89.3113 - val_mae: 7.4567\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 240.1899 - mae: 11.5850 - val_loss: 87.4675 - val_mae: 7.5518\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 235.6226 - mae: 11.2297 - val_loss: 86.4006 - val_mae: 7.6448\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 240.3015 - mae: 11.4932 - val_loss: 86.0616 - val_mae: 7.7797\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 243.8401 - mae: 11.6813 - val_loss: 86.3752 - val_mae: 7.9557\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 242.4735 - mae: 11.6856 - val_loss: 87.1743 - val_mae: 8.1111\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 231.5853 - mae: 11.6452 - val_loss: 88.3981 - val_mae: 8.2600\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 243.9165 - mae: 11.8173 - val_loss: 89.9463 - val_mae: 8.4281\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 227.8504 - mae: 11.5428 - val_loss: 91.8349 - val_mae: 8.6434\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 247.5472 - mae: 12.1567 - val_loss: 94.0840 - val_mae: 8.8576\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 242.8750 - mae: 11.9518 - val_loss: 96.1017 - val_mae: 9.0252\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 245.3784 - mae: 12.0654 - val_loss: 97.6964 - val_mae: 9.1461\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 236.8958 - mae: 11.9063 - val_loss: 98.4668 - val_mae: 9.2015\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 249.9400 - mae: 12.3803 - val_loss: 98.3686 - val_mae: 9.1945\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 246.2441 - mae: 12.0984 - val_loss: 97.5964 - val_mae: 9.1387\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 244.9683 - mae: 12.0559 - val_loss: 96.3879 - val_mae: 9.0475\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 225.3643 - mae: 11.6413 - val_loss: 95.0293 - val_mae: 8.9384\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 248.1554 - mae: 12.2589 - val_loss: 93.6345 - val_mae: 8.8174\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 239.9446 - mae: 11.8676 - val_loss: 92.0303 - val_mae: 8.6635\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 243.5027 - mae: 11.9480 - val_loss: 90.6283 - val_mae: 8.5109\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 240.7722 - mae: 11.7409 - val_loss: 89.5408 - val_mae: 8.3755\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 236.7567 - mae: 11.5124 - val_loss: 88.6973 - val_mae: 8.2915\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 236.6654 - mae: 11.5169 - val_loss: 88.3751 - val_mae: 8.2595\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 226.4429 - mae: 11.4093 - val_loss: 88.4246 - val_mae: 8.2648\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 242.7237 - mae: 11.7592 - val_loss: 88.7193 - val_mae: 8.2941\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 245.9897 - mae: 11.9625 - val_loss: 88.9625 - val_mae: 8.3171\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 243.6237 - mae: 11.8406 - val_loss: 89.0148 - val_mae: 8.3220\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 235.2421 - mae: 11.5464 - val_loss: 88.8309 - val_mae: 8.3051\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 241.7535 - mae: 11.7807 - val_loss: 88.4621 - val_mae: 8.2692\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 241.8989 - mae: 11.8095 - val_loss: 88.1234 - val_mae: 8.2338\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 233.3829 - mae: 11.5211 - val_loss: 87.9052 - val_mae: 8.2095\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 238.2012 - mae: 11.5760 - val_loss: 87.5947 - val_mae: 8.1722\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 239.8473 - mae: 11.6976 - val_loss: 87.3427 - val_mae: 8.1390\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 227.4914 - mae: 11.4185 - val_loss: 87.3526 - val_mae: 8.1405\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 230.0565 - mae: 11.5108 - val_loss: 87.6616 - val_mae: 8.1810\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 219.3078 - mae: 11.1022 - val_loss: 88.1219 - val_mae: 8.2346\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 229.9997 - mae: 11.5573 - val_loss: 88.8822 - val_mae: 8.3113\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 241.1210 - mae: 11.7305 - val_loss: 89.9333 - val_mae: 8.4265\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 226.1482 - mae: 11.2847 - val_loss: 90.5316 - val_mae: 8.4995\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 232.6986 - mae: 11.5285 - val_loss: 90.4132 - val_mae: 8.4855\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 244.3178 - mae: 11.9117 - val_loss: 90.0660 - val_mae: 8.4432\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 231.5937 - mae: 11.6459 - val_loss: 90.0617 - val_mae: 8.4427\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 239.6840 - mae: 11.7525 - val_loss: 90.1925 - val_mae: 8.4588\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 229.2728 - mae: 11.4139 - val_loss: 89.8598 - val_mae: 8.4172\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 241.3263 - mae: 11.7825 - val_loss: 89.4367 - val_mae: 8.3621\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 243.2782 - mae: 11.8155 - val_loss: 88.9739 - val_mae: 8.3213\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 240.4558 - mae: 11.6629 - val_loss: 88.5680 - val_mae: 8.2828\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 243.1518 - mae: 11.7846 - val_loss: 88.2637 - val_mae: 8.2518\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 240.4395 - mae: 11.6762 - val_loss: 87.8509 - val_mae: 8.2061\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 239.2007 - mae: 11.6636 - val_loss: 87.4939 - val_mae: 8.1622\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 238.3634 - mae: 11.6400 - val_loss: 87.1204 - val_mae: 8.1099\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 237.3679 - mae: 11.6275 - val_loss: 86.6671 - val_mae: 8.0321\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 231.6358 - mae: 11.5680 - val_loss: 86.3577 - val_mae: 7.9601\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 246.6857 - mae: 11.8732 - val_loss: 86.2392 - val_mae: 7.9231\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 242.1498 - mae: 11.6404 - val_loss: 86.1795 - val_mae: 7.9002\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 247.0499 - mae: 11.8787 - val_loss: 86.1527 - val_mae: 7.8884\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 255.1690 - mae: 10.0738\n",
            "Test Loss: 255.16900634765625\n",
            "Test Mean Absolute Error: 10.073841094970703\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Training Mean Squared Error (ANN): 238.38443240971517\n",
            "Validation Mean Squared Error (ANN): 86.1526954193893\n",
            "Testing Mean Squared Error (ANN): 255.16899627822986\n",
            "Training Mean Absolute Error (ANN): 11.564197052849662\n",
            "Validation Mean Absolute Error (ANN): 7.888449033101399\n",
            "Testing Mean Absolute Error (ANN): 10.073840713500976\n",
            "Training Mean Squared Error (SVM): 230.4157175230663\n",
            "Validation Mean Squared Error (SVM): 94.16534929715353\n",
            "Testing Mean Squared Error (SVM): 231.83940057599807\n",
            "Training Mean Absolute Error (SVM): 11.107882955135006\n",
            "Validation Mean Absolute Error (SVM): 8.885107729900293\n",
            "Testing Mean Absolute Error (SVM): 9.65040179678344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data[['DBH_m_', 'VH_mean']]\n",
        "y = data['canopy_cir']\n",
        "\n",
        "# Splitting the data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# XGBoost\n",
        "# Convert to DMatrix format\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',  # For regression\n",
        "    'max_depth': 3,\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 100,\n",
        "    'seed': 42\n",
        "}\n",
        "\n",
        "# Train the model\n",
        "model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(dtrain)\n",
        "valid_predictions = model.predict(dvalid)\n",
        "test_predictions = model.predict(dtest)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (XGBoost): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (XGBoost): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (XGBoost): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (XGBoost): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (XGBoost): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (XGBoost): {test_mae}')\n",
        "\n",
        "\n",
        "# Random Forest\n",
        "# Create and train the Random Forest model\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = rf_model.predict(X_train)\n",
        "valid_predictions = rf_model.predict(X_valid)\n",
        "test_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (Random Forest): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (Random Forest): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (Random Forest): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')\n",
        "\n",
        "\n",
        "# ANN\n",
        "# Create the ANN model\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Mean Absolute Error: {mae}')\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = model.predict(X_train).flatten()\n",
        "valid_predictions = model.predict(X_valid).flatten()\n",
        "test_predictions = model.predict(X_test).flatten()\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (ANN): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (ANN): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (ANN): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (ANN): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (ANN): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (ANN): {test_mae}')\n",
        "\n",
        "\n",
        "# SVM\n",
        "# Create and train the SVM model\n",
        "svm_model = SVR(kernel='rbf')  # You can experiment with different kernels\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "train_predictions = svm_model.predict(X_train)\n",
        "valid_predictions = svm_model.predict(X_valid)\n",
        "test_predictions = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate training, validation, and testing errors\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(f'Training Mean Squared Error (SVM): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (SVM): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (SVM): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (SVM): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (SVM): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (SVM): {test_mae}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZPEQ2T1UKC3",
        "outputId": "934fe218-c1de-429f-edf5-6fd8d4c2cd11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\tvalidation-rmse:9.82897\n",
            "[1]\tvalidation-rmse:9.80412\n",
            "[2]\tvalidation-rmse:9.88104\n",
            "[3]\tvalidation-rmse:10.07259\n",
            "[4]\tvalidation-rmse:10.17146\n",
            "[5]\tvalidation-rmse:10.42899\n",
            "[6]\tvalidation-rmse:10.59144\n",
            "[7]\tvalidation-rmse:10.75045\n",
            "[8]\tvalidation-rmse:10.72751\n",
            "[9]\tvalidation-rmse:10.75356\n",
            "[10]\tvalidation-rmse:10.78808\n",
            "[11]\tvalidation-rmse:10.83245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [15:28:24] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"n_estimators\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Mean Squared Error (XGBoost): 127.64418378901726\n",
            "Validation Mean Squared Error (XGBoost): 117.34204163802981\n",
            "Testing Mean Squared Error (XGBoost): 230.44276721169135\n",
            "Training Mean Absolute Error (XGBoost): 8.761395973629421\n",
            "Validation Mean Absolute Error (XGBoost): 9.32128651936849\n",
            "Testing Mean Absolute Error (XGBoost): 9.859400049845378\n",
            "Training Mean Squared Error (Random Forest): 38.822739388888884\n",
            "Validation Mean Squared Error (Random Forest): 158.7185544166666\n",
            "Testing Mean Squared Error (Random Forest): 289.6784519166665\n",
            "Training Mean Absolute Error (Random Forest): 5.119777777777778\n",
            "Validation Mean Absolute Error (Random Forest): 10.071916666666661\n",
            "Testing Mean Absolute Error (Random Forest): 11.805416666666664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 288ms/step - loss: 1677.2031 - mae: 38.1073 - val_loss: 1251.1078 - val_mae: 34.0809\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1600.3484 - mae: 37.0176 - val_loss: 1188.2916 - val_mae: 33.1487\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1488.0431 - mae: 35.4396 - val_loss: 1127.8862 - val_mae: 32.2300\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1415.6714 - mae: 34.4112 - val_loss: 1069.8494 - val_mae: 31.3211\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1342.5851 - mae: 33.4981 - val_loss: 1011.6694 - val_mae: 30.3841\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1279.8893 - mae: 32.5960 - val_loss: 954.2355 - val_mae: 29.4292\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 1248.3300 - mae: 31.8465 - val_loss: 898.9720 - val_mae: 28.4773\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1187.8087 - mae: 30.9289 - val_loss: 846.2867 - val_mae: 27.5420\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1146.7911 - mae: 30.2495 - val_loss: 795.6264 - val_mae: 26.6134\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1046.7791 - mae: 28.7802 - val_loss: 746.6025 - val_mae: 25.6849\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1016.0477 - mae: 28.0160 - val_loss: 698.4258 - val_mae: 24.7409\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 927.1385 - mae: 26.6638 - val_loss: 651.1053 - val_mae: 23.7787\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 918.9423 - mae: 26.5688 - val_loss: 605.0244 - val_mae: 22.8003\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 878.0876 - mae: 25.7251 - val_loss: 561.0574 - val_mae: 21.8256\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 826.2880 - mae: 24.6966 - val_loss: 519.1269 - val_mae: 20.8527\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 757.3325 - mae: 23.4689 - val_loss: 478.3335 - val_mae: 19.8624\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 739.8346 - mae: 23.1919 - val_loss: 439.1469 - val_mae: 18.8620\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 681.0574 - mae: 21.9930 - val_loss: 401.9386 - val_mae: 17.8611\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 642.5137 - mae: 21.1586 - val_loss: 366.0945 - val_mae: 16.8414\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 609.8926 - mae: 20.4408 - val_loss: 332.1308 - val_mae: 15.8155\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 572.8438 - mae: 19.7717 - val_loss: 300.6349 - val_mae: 14.8015\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 503.1400 - mae: 18.0971 - val_loss: 270.5426 - val_mae: 13.7643\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 481.6258 - mae: 17.5561 - val_loss: 240.8385 - val_mae: 12.6638\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 452.0372 - mae: 16.8577 - val_loss: 213.0988 - val_mae: 11.5408\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 401.0346 - mae: 15.8124 - val_loss: 187.4849 - val_mae: 10.4484\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 396.8747 - mae: 15.5378 - val_loss: 164.5049 - val_mae: 9.4959\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 350.6957 - mae: 14.1480 - val_loss: 144.5965 - val_mae: 8.6176\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 331.6056 - mae: 13.6304 - val_loss: 127.3368 - val_mae: 7.8799\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 297.4043 - mae: 12.9071 - val_loss: 112.6490 - val_mae: 7.4180\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 292.0061 - mae: 12.5409 - val_loss: 100.8691 - val_mae: 7.1519\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 261.4574 - mae: 11.9424 - val_loss: 91.6462 - val_mae: 6.9751\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 241.5112 - mae: 11.4136 - val_loss: 84.2006 - val_mae: 7.1098\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 236.6634 - mae: 11.3835 - val_loss: 78.7928 - val_mae: 7.2633\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 220.1447 - mae: 10.8564 - val_loss: 75.6453 - val_mae: 7.4156\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 219.8364 - mae: 11.0265 - val_loss: 74.7094 - val_mae: 7.5632\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 216.5219 - mae: 10.7860 - val_loss: 75.7937 - val_mae: 7.7062\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 214.2296 - mae: 10.7948 - val_loss: 78.6521 - val_mae: 7.8424\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 210.2848 - mae: 11.0792 - val_loss: 82.7454 - val_mae: 8.1444\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 203.9147 - mae: 10.9400 - val_loss: 87.7005 - val_mae: 8.5088\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 208.3905 - mae: 11.2368 - val_loss: 93.4329 - val_mae: 8.8506\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 221.3711 - mae: 11.7809 - val_loss: 99.1078 - val_mae: 9.1396\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 226.1083 - mae: 12.1665 - val_loss: 103.7922 - val_mae: 9.3528\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 231.1576 - mae: 12.4007 - val_loss: 107.2815 - val_mae: 9.5001\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 221.2436 - mae: 12.1917 - val_loss: 108.1723 - val_mae: 9.5352\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 231.3423 - mae: 12.4317 - val_loss: 107.0143 - val_mae: 9.4858\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 227.2180 - mae: 12.2682 - val_loss: 105.0170 - val_mae: 9.3997\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 230.4557 - mae: 12.3682 - val_loss: 102.1780 - val_mae: 9.2734\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 221.4921 - mae: 12.2092 - val_loss: 99.1790 - val_mae: 9.1336\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 211.0610 - mae: 11.6731 - val_loss: 97.1213 - val_mae: 9.0327\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 210.8513 - mae: 11.6557 - val_loss: 94.3211 - val_mae: 8.8889\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 213.3182 - mae: 11.6610 - val_loss: 90.7501 - val_mae: 8.6913\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 207.5060 - mae: 11.3363 - val_loss: 87.4653 - val_mae: 8.4911\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 209.5107 - mae: 11.3297 - val_loss: 84.7328 - val_mae: 8.3063\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 221.8450 - mae: 11.6178 - val_loss: 82.7944 - val_mae: 8.1617\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 216.4984 - mae: 11.2420 - val_loss: 81.2306 - val_mae: 8.0344\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 221.6191 - mae: 11.4699 - val_loss: 80.0837 - val_mae: 7.9456\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 221.5418 - mae: 11.4310 - val_loss: 78.9801 - val_mae: 7.8751\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 215.0718 - mae: 11.0067 - val_loss: 78.1285 - val_mae: 7.8162\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 215.0613 - mae: 11.0192 - val_loss: 77.7570 - val_mae: 7.7947\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 222.1888 - mae: 11.3582 - val_loss: 77.6230 - val_mae: 7.7896\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 198.3666 - mae: 10.6473 - val_loss: 77.6123 - val_mae: 7.7887\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 200.7695 - mae: 10.8009 - val_loss: 77.7961 - val_mae: 7.8003\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 220.8816 - mae: 11.2867 - val_loss: 77.8741 - val_mae: 7.8092\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 219.4458 - mae: 11.2057 - val_loss: 77.8180 - val_mae: 7.8076\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 220.6561 - mae: 11.2689 - val_loss: 77.7188 - val_mae: 7.8027\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 220.6979 - mae: 11.2513 - val_loss: 77.3929 - val_mae: 7.7800\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 210.1630 - mae: 10.8906 - val_loss: 76.8107 - val_mae: 7.7561\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 214.6422 - mae: 10.9706 - val_loss: 76.1305 - val_mae: 7.7289\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 196.0365 - mae: 10.4554 - val_loss: 75.6579 - val_mae: 7.7084\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 215.2832 - mae: 10.9298 - val_loss: 75.2128 - val_mae: 7.6878\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 211.3715 - mae: 10.8667 - val_loss: 74.5983 - val_mae: 7.6567\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 217.4870 - mae: 11.0346 - val_loss: 74.0207 - val_mae: 7.6232\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 204.7493 - mae: 10.6732 - val_loss: 73.7304 - val_mae: 7.6046\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 205.2710 - mae: 10.6439 - val_loss: 73.9439 - val_mae: 7.6199\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 219.3363 - mae: 11.0982 - val_loss: 74.5309 - val_mae: 7.6534\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 212.4278 - mae: 10.7920 - val_loss: 75.5499 - val_mae: 7.6995\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 200.7773 - mae: 10.6003 - val_loss: 77.3235 - val_mae: 7.8174\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 213.1258 - mae: 10.9542 - val_loss: 79.4331 - val_mae: 7.9628\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 218.3137 - mae: 11.2653 - val_loss: 80.9882 - val_mae: 8.0567\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 216.0397 - mae: 11.2601 - val_loss: 81.4735 - val_mae: 8.0854\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 207.5206 - mae: 11.1035 - val_loss: 81.1018 - val_mae: 8.0667\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 195.6231 - mae: 10.8192 - val_loss: 81.1165 - val_mae: 8.0694\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 219.4279 - mae: 11.4896 - val_loss: 81.6249 - val_mae: 8.0988\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 217.5275 - mae: 11.4299 - val_loss: 81.9557 - val_mae: 8.1177\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 216.2807 - mae: 11.3517 - val_loss: 82.4366 - val_mae: 8.1439\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 208.3886 - mae: 11.1156 - val_loss: 82.4096 - val_mae: 8.1441\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 214.2359 - mae: 11.2771 - val_loss: 81.7617 - val_mae: 8.1124\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 215.7619 - mae: 11.2994 - val_loss: 81.3019 - val_mae: 8.0898\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 215.1662 - mae: 11.1931 - val_loss: 81.2045 - val_mae: 8.0865\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 219.1756 - mae: 11.4495 - val_loss: 80.9287 - val_mae: 8.0738\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 212.3022 - mae: 11.1352 - val_loss: 80.3246 - val_mae: 8.0426\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 217.6088 - mae: 11.3302 - val_loss: 79.5210 - val_mae: 7.9985\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 214.7776 - mae: 11.2771 - val_loss: 78.4623 - val_mae: 7.9363\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 214.6890 - mae: 11.2352 - val_loss: 77.2675 - val_mae: 7.8602\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 209.4330 - mae: 10.9365 - val_loss: 75.7951 - val_mae: 7.7542\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 207.6476 - mae: 10.6892 - val_loss: 74.4607 - val_mae: 7.6419\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 218.5252 - mae: 11.1013 - val_loss: 73.7241 - val_mae: 7.6083\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 214.8917 - mae: 10.9040 - val_loss: 73.3624 - val_mae: 7.5900\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 212.5177 - mae: 10.7606 - val_loss: 73.1775 - val_mae: 7.5801\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 210.2142 - mae: 10.8114 - val_loss: 72.8647 - val_mae: 7.5626\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 228.5720 - mae: 10.0062\n",
            "Test Loss: 228.57196044921875\n",
            "Test Mean Absolute Error: 10.006196975708008\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "Training Mean Squared Error (ANN): 212.61582547432434\n",
            "Validation Mean Squared Error (ANN): 72.8647121693369\n",
            "Testing Mean Squared Error (ANN): 228.5719564250397\n",
            "Training Mean Absolute Error (ANN): 10.828689829508463\n",
            "Validation Mean Absolute Error (ANN): 7.562554359436035\n",
            "Testing Mean Absolute Error (ANN): 10.006196721394856\n",
            "Training Mean Squared Error (SVM): 228.88014172168303\n",
            "Validation Mean Squared Error (SVM): 91.39993744669943\n",
            "Testing Mean Squared Error (SVM): 227.31107501558031\n",
            "Training Mean Absolute Error (SVM): 11.070393381165253\n",
            "Validation Mean Absolute Error (SVM): 8.706635219523557\n",
            "Testing Mean Absolute Error (SVM): 9.543793362189874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: plot training and testing result\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have your test predictions and true test values in the variables 'test_predictions' and 'y_test'\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, test_predictions, alpha=0.5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)  # Diagonal line for perfect prediction\n",
        "plt.xlabel('True Values')\n",
        "plt.ylabel('Predictions')\n",
        "plt.title('True vs. Predicted Values (Test Set)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c4O5PsSdmObi",
        "outputId": "3e3ad457-6ca6-4c32-b33d-f2b57b563282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: tune the above model\n",
        "\n",
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# XGBoost Hyperparameter Tuning\n",
        "X = data[['DBH_m_', 'VH_mean']]\n",
        "y = data['canopy_cir']\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'subsample': [0.8, 1.0],  # Example subsample parameter\n",
        "    'colsample_bytree': [0.8, 1.0]  # Example colsample_bytree parameter\n",
        "}\n",
        "\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', seed=42)\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid,\n",
        "                           scoring='neg_mean_squared_error', cv=3, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "\n",
        "# Train with the best parameters\n",
        "best_xgb_model = xgb.XGBRegressor(**grid_search.best_params_, objective='reg:squarederror', seed=42)\n",
        "best_xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the tuned model\n",
        "train_predictions = best_xgb_model.predict(X_train)\n",
        "valid_predictions = best_xgb_model.predict(X_valid)\n",
        "test_predictions = best_xgb_model.predict(X_test)\n",
        "\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "valid_mse = mean_squared_error(y_valid, valid_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "valid_mae = mean_absolute_error(y_valid, valid_predictions)\n",
        "test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "\n",
        "print(f'Training Mean Squared Error (Tuned XGBoost): {train_mse}')\n",
        "print(f'Validation Mean Squared Error (Tuned XGBoost): {valid_mse}')\n",
        "print(f'Testing Mean Squared Error (Tuned XGBoost): {test_mse}')\n",
        "print(f'Training Mean Absolute Error (Tuned XGBoost): {train_mae}')\n",
        "print(f'Validation Mean Absolute Error (Tuned XGBoost): {valid_mae}')\n",
        "print(f'Testing Mean Absolute Error (Tuned XGBoost): {test_mae}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iTj3dG8VncJ",
        "outputId": "3c32992d-97b4-4f22-b47d-2a351b3be631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
            "Best parameters found:  {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}\n",
            "Training Mean Squared Error (Tuned XGBoost): 107.71477805643872\n",
            "Validation Mean Squared Error (Tuned XGBoost): 85.43230972214103\n",
            "Testing Mean Squared Error (Tuned XGBoost): 194.16043519965635\n",
            "Training Mean Absolute Error (Tuned XGBoost): 7.557528856065537\n",
            "Validation Mean Absolute Error (Tuned XGBoost): 7.963457425435384\n",
            "Testing Mean Absolute Error (Tuned XGBoost): 9.287023289998372\n"
          ]
        }
      ]
    }
  ]
}