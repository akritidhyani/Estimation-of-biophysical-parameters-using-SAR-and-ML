# -*- coding: utf-8 -*-
"""SAR_train_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zGsvnocF8xTbznSAgkRcNt9RDFX34qpj
"""

import xgboost as xgb
import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('/content/File.csv')

# Preprocess the data
# Keep only the VV and VH columns as features
X = data[['VV_mean', 'VH_mean']]
y = data['LAI']

# Splitting the data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Convert to DMatrix format
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)
dtest = xgb.DMatrix(X_test, label=y_test)

# Set parameters
params = {
    'objective': 'reg:squarederror',  # For regression
    'max_depth': 3,
    'learning_rate': 0.1,
    'n_estimators': 100,
    'seed': 42
}

# Train the model
model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)

# Make predictions
train_predictions = model.predict(dtrain)
test_predictions = model.predict(dtest)

# Calculate training and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error: {train_mse}')
print(f'Testing Mean Squared Error: {test_mse}')
print(f'Training Mean Absolute Error: {train_mae}')
print(f'Testing Mean Absolute Error: {test_mae}')

# Visualize feature importance
xgb.plot_importance(model)
plt.title("Feature Importance")
plt.show()

# prompt: algorithm that predicts LAI on the basis of VV and VH. Train the model and also test and validate it

import xgboost as xgb
import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('/content/File.csv')

# Preprocess the data
# Keep only the VV and VH columns as features
X = data[['VV_mean', 'VH_mean']]
y = data['LAI']

# Splitting the data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Convert to DMatrix format
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)
dtest = xgb.DMatrix(X_test, label=y_test)

# Set parameters
params = {
    'objective': 'reg:squarederror',  # For regression
    'max_depth': 3,
    'learning_rate': 0.1,
    'n_estimators': 100,
    'seed': 42
}

# Train the model
model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)

# Make predictions
train_predictions = model.predict(dtrain)
valid_predictions = model.predict(dvalid)
test_predictions = model.predict(dtest)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error: {train_mse}')
print(f'Validation Mean Squared Error: {valid_mse}')
print(f'Testing Mean Squared Error: {test_mse}')
print(f'Training Mean Absolute Error: {train_mae}')
print(f'Validation Mean Absolute Error: {valid_mae}')
print(f'Testing Mean Absolute Error: {test_mae}')

# prompt: try another algorithm

import pandas as pd
from sklearn.ensemble import RandomForestRegressor

# Load the dataset
data = pd.read_csv('/content/File.csv')

# Preprocess the data
# Keep only the VV and VH columns as features
X = data[['VV_mean', 'VH_mean']]
y = data['LAI']

# Splitting the data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Create and train the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
train_predictions = rf_model.predict(X_train)
valid_predictions = rf_model.predict(X_valid)
test_predictions = rf_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (Random Forest): {train_mse}')
print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')
print(f'Testing Mean Squared Error (Random Forest): {test_mse}')
print(f'Training Mean Absolute Error (Random Forest): {train_mae}')
print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')
print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')

# You can also analyze feature importance for Random Forest
importances = rf_model.feature_importances_
feature_names = X.columns
for feature, importance in zip(feature_names, importances):
    print(f"{feature}: {importance}")

# prompt: use ANN

import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Load the dataset
data = pd.read_csv('/content/File.csv')

# Preprocess the data
# Keep only the VV and VH columns as features
X = data[['VV_mean', 'VH_mean']]
y = data['LAI']

# Splitting the data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Create the ANN model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dense(32, activation='relu'),
    layers.Dense(1)  # Output layer for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))

# Evaluate the model
loss, mae = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}')
print(f'Test Mean Absolute Error: {mae}')

# Make predictions
train_predictions = model.predict(X_train).flatten()
valid_predictions = model.predict(X_valid).flatten()
test_predictions = model.predict(X_test).flatten()

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (ANN): {train_mse}')
print(f'Validation Mean Squared Error (ANN): {valid_mse}')
print(f'Testing Mean Squared Error (ANN): {test_mse}')
print(f'Training Mean Absolute Error (ANN): {train_mae}')
print(f'Validation Mean Absolute Error (ANN): {valid_mae}')
print(f'Testing Mean Absolute Error (ANN): {test_mae}')


# Plot the training and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# prompt: try using svm

import xgboost as xgb
import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.svm import SVR

# Load the dataset
data = pd.read_csv('/content/File.csv')

# Preprocess the data
# Keep only the VV and VH columns as features
X = data[['VV_mean', 'VH_mean']]
y = data['LAI']

# Splitting the data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Create and train the SVM model
svm_model = SVR(kernel='rbf')  # You can experiment with different kernels
svm_model.fit(X_train, y_train)

# Make predictions
train_predictions = svm_model.predict(X_train)
valid_predictions = svm_model.predict(X_valid)
test_predictions = svm_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (SVM): {train_mse}')
print(f'Validation Mean Squared Error (SVM): {valid_mse}')
print(f'Testing Mean Squared Error (SVM): {test_mse}')
print(f'Training Mean Absolute Error (SVM): {train_mae}')
print(f'Validation Mean Absolute Error (SVM): {valid_mae}')
print(f'Testing Mean Absolute Error (SVM): {test_mae}')

data.head()

# prompt: provide the same code to predict LAI with input variables ndvi and ndre from the file

import xgboost as xgb
import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.svm import SVR

# Load the dataset
data = pd.read_csv('/content/File.csv')

# Preprocess the data
# Keep only the ndvi and ndre columns as features
X = data[['ndvi2023_mean', 'ndre2023_mean']]
y = data['LAI']

# Splitting the data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# XGBoost
# Convert to DMatrix format
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)
dtest = xgb.DMatrix(X_test, label=y_test)

# Set parameters
params = {
    'objective': 'reg:squarederror',  # For regression
    'max_depth': 3,
    'learning_rate': 0.1,
    'n_estimators': 100,
    'seed': 42
}

# Train the model
model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)

# Make predictions
train_predictions = model.predict(dtrain)
valid_predictions = model.predict(dvalid)
test_predictions = model.predict(dtest)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (XGBoost): {train_mse}')
print(f'Validation Mean Squared Error (XGBoost): {valid_mse}')
print(f'Testing Mean Squared Error (XGBoost): {test_mse}')
print(f'Training Mean Absolute Error (XGBoost): {train_mae}')
print(f'Validation Mean Absolute Error (XGBoost): {valid_mae}')
print(f'Testing Mean Absolute Error (XGBoost): {test_mae}')


# Random Forest
# Create and train the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
train_predictions = rf_model.predict(X_train)
valid_predictions = rf_model.predict(X_valid)
test_predictions = rf_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (Random Forest): {train_mse}')
print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')
print(f'Testing Mean Squared Error (Random Forest): {test_mse}')
print(f'Training Mean Absolute Error (Random Forest): {train_mae}')
print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')
print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')


# ANN
# Create the ANN model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dense(32, activation='relu'),
    layers.Dense(1)  # Output layer for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))

# Evaluate the model
loss, mae = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}')
print(f'Test Mean Absolute Error: {mae}')

# Make predictions
train_predictions = model.predict(X_train).flatten()
valid_predictions = model.predict(X_valid).flatten()
test_predictions = model.predict(X_test).flatten()

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (ANN): {train_mse}')
print(f'Validation Mean Squared Error (ANN): {valid_mse}')
print(f'Testing Mean Squared Error (ANN): {test_mse}')
print(f'Training Mean Absolute Error (ANN): {train_mae}')
print(f'Validation Mean Absolute Error (ANN): {valid_mae}')
print(f'Testing Mean Absolute Error (ANN): {test_mae}')


# SVM
# Create and train the SVM model
svm_model = SVR(kernel='rbf')  # You can experiment with different kernels
svm_model.fit(X_train, y_train)

# Make predictions
train_predictions = svm_model.predict(X_train)
valid_predictions = svm_model.predict(X_valid)
test_predictions = svm_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (SVM): {train_mse}')
print(f'Validation Mean Squared Error (SVM): {valid_mse}')
print(f'Testing Mean Squared Error (SVM): {test_mse}')
print(f'Training Mean Absolute Error (SVM): {train_mae}')
print(f'Validation Mean Absolute Error (SVM): {valid_mae}')
print(f'Testing Mean Absolute Error (SVM): {test_mae}')

X = data[['VV_mean', 'VH_mean']]
y = data['DBH_m_']

# Splitting the data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# XGBoost
# Convert to DMatrix format
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)
dtest = xgb.DMatrix(X_test, label=y_test)

# Set parameters
params = {
    'objective': 'reg:squarederror',  # For regression
    'max_depth': 3,
    'learning_rate': 0.1,
    'n_estimators': 100,
    'seed': 42
}

# Train the model
model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)

# Make predictions
train_predictions = model.predict(dtrain)
valid_predictions = model.predict(dvalid)
test_predictions = model.predict(dtest)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (XGBoost): {train_mse}')
print(f'Validation Mean Squared Error (XGBoost): {valid_mse}')
print(f'Testing Mean Squared Error (XGBoost): {test_mse}')
print(f'Training Mean Absolute Error (XGBoost): {train_mae}')
print(f'Validation Mean Absolute Error (XGBoost): {valid_mae}')
print(f'Testing Mean Absolute Error (XGBoost): {test_mae}')


# Random Forest
# Create and train the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
train_predictions = rf_model.predict(X_train)
valid_predictions = rf_model.predict(X_valid)
test_predictions = rf_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (Random Forest): {train_mse}')
print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')
print(f'Testing Mean Squared Error (Random Forest): {test_mse}')
print(f'Training Mean Absolute Error (Random Forest): {train_mae}')
print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')
print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')


# ANN
# Create the ANN model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dense(32, activation='relu'),
    layers.Dense(1)  # Output layer for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))

# Evaluate the model
loss, mae = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}')
print(f'Test Mean Absolute Error: {mae}')

# Make predictions
train_predictions = model.predict(X_train).flatten()
valid_predictions = model.predict(X_valid).flatten()
test_predictions = model.predict(X_test).flatten()

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (ANN): {train_mse}')
print(f'Validation Mean Squared Error (ANN): {valid_mse}')
print(f'Testing Mean Squared Error (ANN): {test_mse}')
print(f'Training Mean Absolute Error (ANN): {train_mae}')
print(f'Validation Mean Absolute Error (ANN): {valid_mae}')
print(f'Testing Mean Absolute Error (ANN): {test_mae}')


# SVM
# Create and train the SVM model
svm_model = SVR(kernel='rbf')  # You can experiment with different kernels
svm_model.fit(X_train, y_train)

# Make predictions
train_predictions = svm_model.predict(X_train)
valid_predictions = svm_model.predict(X_valid)
test_predictions = svm_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (SVM): {train_mse}')
print(f'Validation Mean Squared Error (SVM): {valid_mse}')
print(f'Testing Mean Squared Error (SVM): {test_mse}')
print(f'Training Mean Absolute Error (SVM): {train_mae}')
print(f'Validation Mean Absolute Error (SVM): {valid_mae}')
print(f'Testing Mean Absolute Error (SVM): {test_mae}')

X = data[['VH_mean', 'Plant_heig']]
y = data['DBH_m_']

# Splitting the data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# XGBoost
# Convert to DMatrix format
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)
dtest = xgb.DMatrix(X_test, label=y_test)

# Set parameters
params = {
    'objective': 'reg:squarederror',  # For regression
    'max_depth': 3,
    'learning_rate': 0.1,
    'n_estimators': 100,
    'seed': 42
}

# Train the model
model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)

# Make predictions
train_predictions = model.predict(dtrain)
valid_predictions = model.predict(dvalid)
test_predictions = model.predict(dtest)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (XGBoost): {train_mse}')
print(f'Validation Mean Squared Error (XGBoost): {valid_mse}')
print(f'Testing Mean Squared Error (XGBoost): {test_mse}')
print(f'Training Mean Absolute Error (XGBoost): {train_mae}')
print(f'Validation Mean Absolute Error (XGBoost): {valid_mae}')
print(f'Testing Mean Absolute Error (XGBoost): {test_mae}')


# Random Forest
# Create and train the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
train_predictions = rf_model.predict(X_train)
valid_predictions = rf_model.predict(X_valid)
test_predictions = rf_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (Random Forest): {train_mse}')
print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')
print(f'Testing Mean Squared Error (Random Forest): {test_mse}')
print(f'Training Mean Absolute Error (Random Forest): {train_mae}')
print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')
print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')

# ANN
# Create the ANN model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dense(32, activation='relu'),
    layers.Dense(1)  # Output layer for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))

# Evaluate the model
loss, mae = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}')
print(f'Test Mean Absolute Error: {mae}')

# Make predictions
train_predictions = model.predict(X_train).flatten()
valid_predictions = model.predict(X_valid).flatten()
test_predictions = model.predict(X_test).flatten()

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (ANN): {train_mse}')
print(f'Validation Mean Squared Error (ANN): {valid_mse}')
print(f'Testing Mean Squared Error (ANN): {test_mse}')
print(f'Training Mean Absolute Error (ANN): {train_mae}')
print(f'Validation Mean Absolute Error (ANN): {valid_mae}')
print(f'Testing Mean Absolute Error (ANN): {test_mae}')


# SVM
# Create and train the SVM model
svm_model = SVR(kernel='rbf')  # You can experiment with different kernels
svm_model.fit(X_train, y_train)

# Make predictions
train_predictions = svm_model.predict(X_train)
valid_predictions = svm_model.predict(X_valid)
test_predictions = svm_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (SVM): {train_mse}')
print(f'Validation Mean Squared Error (SVM): {valid_mse}')
print(f'Testing Mean Squared Error (SVM): {test_mse}')
print(f'Training Mean Absolute Error (SVM): {train_mae}')
print(f'Validation Mean Absolute Error (SVM): {valid_mae}')
print(f'Testing Mean Absolute Error (SVM): {test_mae}')

X = data[['VH_mean', 'Plant_heig']]
y = data['DBH_m_']

# Splitting the data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# XGBoost
# Convert to DMatrix format
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)
dtest = xgb.DMatrix(X_test, label=y_test)

# Set parameters
params = {
    'objective': 'reg:squarederror',  # For regression
    'max_depth': 3,
    'learning_rate': 0.1,
    'n_estimators': 100,
    'seed': 42
}

# Train the model
model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)

# Make predictions
train_predictions = model.predict(dtrain)
valid_predictions = model.predict(dvalid)
test_predictions = model.predict(dtest)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (XGBoost): {train_mse}')
print(f'Validation Mean Squared Error (XGBoost): {valid_mse}')
print(f'Testing Mean Squared Error (XGBoost): {test_mse}')
print(f'Training Mean Absolute Error (XGBoost): {train_mae}')
print(f'Validation Mean Absolute Error (XGBoost): {valid_mae}')
print(f'Testing Mean Absolute Error (XGBoost): {test_mae}')


# Random Forest
# Create and train the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
train_predictions = rf_model.predict(X_train)
valid_predictions = rf_model.predict(X_valid)
test_predictions = rf_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (Random Forest): {train_mse}')
print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')
print(f'Testing Mean Squared Error (Random Forest): {test_mse}')
print(f'Training Mean Absolute Error (Random Forest): {train_mae}')
print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')
print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')


# ANN
# Create the ANN model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dense(32, activation='relu'),
    layers.Dense(1)  # Output layer for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))

# Evaluate the model
loss, mae = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}')
print(f'Test Mean Absolute Error: {mae}')

# Make predictions
train_predictions = model.predict(X_train).flatten()
valid_predictions = model.predict(X_valid).flatten()
test_predictions = model.predict(X_test).flatten()

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (ANN): {train_mse}')
print(f'Validation Mean Squared Error (ANN): {valid_mse}')
print(f'Testing Mean Squared Error (ANN): {test_mse}')
print(f'Training Mean Absolute Error (ANN): {train_mae}')
print(f'Validation Mean Absolute Error (ANN): {valid_mae}')
print(f'Testing Mean Absolute Error (ANN): {test_mae}')


# SVM
# Create and train the SVM model
svm_model = SVR(kernel='rbf')  # You can experiment with different kernels
svm_model.fit(X_train, y_train)

# Make predictions
train_predictions = svm_model.predict(X_train)
valid_predictions = svm_model.predict(X_valid)
test_predictions = svm_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (SVM): {train_mse}')
print(f'Validation Mean Squared Error (SVM): {valid_mse}')
print(f'Testing Mean Squared Error (SVM): {test_mse}')
print(f'Training Mean Absolute Error (SVM): {train_mae}')
print(f'Validation Mean Absolute Error (SVM): {valid_mae}')
print(f'Testing Mean Absolute Error (SVM): {test_mae}')

X = data[['VV_mean', 'VH_mean']]
y = data['Plant_heig']

# Splitting the data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# XGBoost
# Convert to DMatrix format
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)
dtest = xgb.DMatrix(X_test, label=y_test)

# Set parameters
params = {
    'objective': 'reg:squarederror',  # For regression
    'max_depth': 3,
    'learning_rate': 0.1,
    'n_estimators': 100,
    'seed': 42
}

# Train the model
model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)

# Make predictions
train_predictions = model.predict(dtrain)
valid_predictions = model.predict(dvalid)
test_predictions = model.predict(dtest)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (XGBoost): {train_mse}')
print(f'Validation Mean Squared Error (XGBoost): {valid_mse}')
print(f'Testing Mean Squared Error (XGBoost): {test_mse}')
print(f'Training Mean Absolute Error (XGBoost): {train_mae}')
print(f'Validation Mean Absolute Error (XGBoost): {valid_mae}')
print(f'Testing Mean Absolute Error (XGBoost): {test_mae}')


# Random Forest
# Create and train the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
train_predictions = rf_model.predict(X_train)
valid_predictions = rf_model.predict(X_valid)
test_predictions = rf_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (Random Forest): {train_mse}')
print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')
print(f'Testing Mean Squared Error (Random Forest): {test_mse}')
print(f'Training Mean Absolute Error (Random Forest): {train_mae}')
print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')
print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')


# ANN
# Create the ANN model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dense(32, activation='relu'),
    layers.Dense(1)  # Output layer for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))

# Evaluate the model
loss, mae = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}')
print(f'Test Mean Absolute Error: {mae}')

# Make predictions
train_predictions = model.predict(X_train).flatten()
valid_predictions = model.predict(X_valid).flatten()
test_predictions = model.predict(X_test).flatten()

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (ANN): {train_mse}')
print(f'Validation Mean Squared Error (ANN): {valid_mse}')
print(f'Testing Mean Squared Error (ANN): {test_mse}')
print(f'Training Mean Absolute Error (ANN): {train_mae}')
print(f'Validation Mean Absolute Error (ANN): {valid_mae}')
print(f'Testing Mean Absolute Error (ANN): {test_mae}')


# SVM
# Create and train the SVM model
svm_model = SVR(kernel='rbf')  # You can experiment with different kernels
svm_model.fit(X_train, y_train)

# Make predictions
train_predictions = svm_model.predict(X_train)
valid_predictions = svm_model.predict(X_valid)
test_predictions = svm_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (SVM): {train_mse}')
print(f'Validation Mean Squared Error (SVM): {valid_mse}')
print(f'Testing Mean Squared Error (SVM): {test_mse}')
print(f'Training Mean Absolute Error (SVM): {train_mae}')
print(f'Validation Mean Absolute Error (SVM): {valid_mae}')
print(f'Testing Mean Absolute Error (SVM): {test_mae}')

X = data[['VH_mean', 'DBH_m_']]
y = data['Plant_heig']

# Splitting the data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# XGBoost
# Convert to DMatrix format
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)
dtest = xgb.DMatrix(X_test, label=y_test)

# Set parameters
params = {
    'objective': 'reg:squarederror',  # For regression
    'max_depth': 3,
    'learning_rate': 0.1,
    'n_estimators': 100,
    'seed': 42
}

# Train the model
model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)

# Make predictions
train_predictions = model.predict(dtrain)
valid_predictions = model.predict(dvalid)
test_predictions = model.predict(dtest)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (XGBoost): {train_mse}')
print(f'Validation Mean Squared Error (XGBoost): {valid_mse}')
print(f'Testing Mean Squared Error (XGBoost): {test_mse}')
print(f'Training Mean Absolute Error (XGBoost): {train_mae}')
print(f'Validation Mean Absolute Error (XGBoost): {valid_mae}')
print(f'Testing Mean Absolute Error (XGBoost): {test_mae}')


# Random Forest
# Create and train the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
train_predictions = rf_model.predict(X_train)
valid_predictions = rf_model.predict(X_valid)
test_predictions = rf_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (Random Forest): {train_mse}')
print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')
print(f'Testing Mean Squared Error (Random Forest): {test_mse}')
print(f'Training Mean Absolute Error (Random Forest): {train_mae}')
print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')
print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')


# ANN
# Create the ANN model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dense(32, activation='relu'),
    layers.Dense(1)  # Output layer for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))

# Evaluate the model
loss, mae = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}')
print(f'Test Mean Absolute Error: {mae}')

# Make predictions
train_predictions = model.predict(X_train).flatten()
valid_predictions = model.predict(X_valid).flatten()
test_predictions = model.predict(X_test).flatten()

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (ANN): {train_mse}')
print(f'Validation Mean Squared Error (ANN): {valid_mse}')
print(f'Testing Mean Squared Error (ANN): {test_mse}')
print(f'Training Mean Absolute Error (ANN): {train_mae}')
print(f'Validation Mean Absolute Error (ANN): {valid_mae}')
print(f'Testing Mean Absolute Error (ANN): {test_mae}')


# SVM
# Create and train the SVM model
svm_model = SVR(kernel='rbf')  # You can experiment with different kernels
svm_model.fit(X_train, y_train)

# Make predictions
train_predictions = svm_model.predict(X_train)
valid_predictions = svm_model.predict(X_valid)
test_predictions = svm_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (SVM): {train_mse}')
print(f'Validation Mean Squared Error (SVM): {valid_mse}')
print(f'Testing Mean Squared Error (SVM): {test_mse}')
print(f'Training Mean Absolute Error (SVM): {train_mae}')
print(f'Validation Mean Absolute Error (SVM): {valid_mae}')
print(f'Testing Mean Absolute Error (SVM): {test_mae}')

X = data[['VV_mean', 'DBH_m_']]
y = data['Plant_heig']

# Splitting the data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# XGBoost
# Convert to DMatrix format
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)
dtest = xgb.DMatrix(X_test, label=y_test)

# Set parameters
params = {
    'objective': 'reg:squarederror',  # For regression
    'max_depth': 3,
    'learning_rate': 0.1,
    'n_estimators': 100,
    'seed': 42
}

# Train the model
model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)

# Make predictions
train_predictions = model.predict(dtrain)
valid_predictions = model.predict(dvalid)
test_predictions = model.predict(dtest)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (XGBoost): {train_mse}')
print(f'Validation Mean Squared Error (XGBoost): {valid_mse}')
print(f'Testing Mean Squared Error (XGBoost): {test_mse}')
print(f'Training Mean Absolute Error (XGBoost): {train_mae}')
print(f'Validation Mean Absolute Error (XGBoost): {valid_mae}')
print(f'Testing Mean Absolute Error (XGBoost): {test_mae}')


# Random Forest
# Create and train the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
train_predictions = rf_model.predict(X_train)
valid_predictions = rf_model.predict(X_valid)
test_predictions = rf_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (Random Forest): {train_mse}')
print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')
print(f'Testing Mean Squared Error (Random Forest): {test_mse}')
print(f'Training Mean Absolute Error (Random Forest): {train_mae}')
print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')
print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')


# ANN
# Create the ANN model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dense(32, activation='relu'),
    layers.Dense(1)  # Output layer for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))

# Evaluate the model
loss, mae = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}')
print(f'Test Mean Absolute Error: {mae}')

# Make predictions
train_predictions = model.predict(X_train).flatten()
valid_predictions = model.predict(X_valid).flatten()
test_predictions = model.predict(X_test).flatten()

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (ANN): {train_mse}')
print(f'Validation Mean Squared Error (ANN): {valid_mse}')
print(f'Testing Mean Squared Error (ANN): {test_mse}')
print(f'Training Mean Absolute Error (ANN): {train_mae}')
print(f'Validation Mean Absolute Error (ANN): {valid_mae}')
print(f'Testing Mean Absolute Error (ANN): {test_mae}')


# SVM
# Create and train the SVM model
svm_model = SVR(kernel='rbf')  # You can experiment with different kernels
svm_model.fit(X_train, y_train)

# Make predictions
train_predictions = svm_model.predict(X_train)
valid_predictions = svm_model.predict(X_valid)
test_predictions = svm_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (SVM): {train_mse}')
print(f'Validation Mean Squared Error (SVM): {valid_mse}')
print(f'Testing Mean Squared Error (SVM): {test_mse}')
print(f'Training Mean Absolute Error (SVM): {train_mae}')
print(f'Validation Mean Absolute Error (SVM): {valid_mae}')
print(f'Testing Mean Absolute Error (SVM): {test_mae}')

X = data[['VV_mean', 'VH_mean']]
y = data['canopy_cir']

# Splitting the data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# XGBoost
# Convert to DMatrix format
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)
dtest = xgb.DMatrix(X_test, label=y_test)

# Set parameters
params = {
    'objective': 'reg:squarederror',  # For regression
    'max_depth': 3,
    'learning_rate': 0.1,
    'n_estimators': 100,
    'seed': 42
}

# Train the model
model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)

# Make predictions
train_predictions = model.predict(dtrain)
valid_predictions = model.predict(dvalid)
test_predictions = model.predict(dtest)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (XGBoost): {train_mse}')
print(f'Validation Mean Squared Error (XGBoost): {valid_mse}')
print(f'Testing Mean Squared Error (XGBoost): {test_mse}')
print(f'Training Mean Absolute Error (XGBoost): {train_mae}')
print(f'Validation Mean Absolute Error (XGBoost): {valid_mae}')
print(f'Testing Mean Absolute Error (XGBoost): {test_mae}')


# Random Forest
# Create and train the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
train_predictions = rf_model.predict(X_train)
valid_predictions = rf_model.predict(X_valid)
test_predictions = rf_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (Random Forest): {train_mse}')
print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')
print(f'Testing Mean Squared Error (Random Forest): {test_mse}')
print(f'Training Mean Absolute Error (Random Forest): {train_mae}')
print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')
print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')


# ANN
# Create the ANN model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dense(32, activation='relu'),
    layers.Dense(1)  # Output layer for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))

# Evaluate the model
loss, mae = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}')
print(f'Test Mean Absolute Error: {mae}')

# Make predictions
train_predictions = model.predict(X_train).flatten()
valid_predictions = model.predict(X_valid).flatten()
test_predictions = model.predict(X_test).flatten()

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (ANN): {train_mse}')
print(f'Validation Mean Squared Error (ANN): {valid_mse}')
print(f'Testing Mean Squared Error (ANN): {test_mse}')
print(f'Training Mean Absolute Error (ANN): {train_mae}')
print(f'Validation Mean Absolute Error (ANN): {valid_mae}')
print(f'Testing Mean Absolute Error (ANN): {test_mae}')


# SVM
# Create and train the SVM model
svm_model = SVR(kernel='rbf')  # You can experiment with different kernels
svm_model.fit(X_train, y_train)

# Make predictions
train_predictions = svm_model.predict(X_train)
valid_predictions = svm_model.predict(X_valid)
test_predictions = svm_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (SVM): {train_mse}')
print(f'Validation Mean Squared Error (SVM): {valid_mse}')
print(f'Testing Mean Squared Error (SVM): {test_mse}')
print(f'Training Mean Absolute Error (SVM): {train_mae}')
print(f'Validation Mean Absolute Error (SVM): {valid_mae}')
print(f'Testing Mean Absolute Error (SVM): {test_mae}')

X = data[['DBH_m_', 'VH_mean']]
y = data['canopy_cir']

# Splitting the data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# XGBoost
# Convert to DMatrix format
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)
dtest = xgb.DMatrix(X_test, label=y_test)

# Set parameters
params = {
    'objective': 'reg:squarederror',  # For regression
    'max_depth': 3,
    'learning_rate': 0.1,
    'n_estimators': 100,
    'seed': 42
}

# Train the model
model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dvalid, 'validation')], early_stopping_rounds=10)

# Make predictions
train_predictions = model.predict(dtrain)
valid_predictions = model.predict(dvalid)
test_predictions = model.predict(dtest)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (XGBoost): {train_mse}')
print(f'Validation Mean Squared Error (XGBoost): {valid_mse}')
print(f'Testing Mean Squared Error (XGBoost): {test_mse}')
print(f'Training Mean Absolute Error (XGBoost): {train_mae}')
print(f'Validation Mean Absolute Error (XGBoost): {valid_mae}')
print(f'Testing Mean Absolute Error (XGBoost): {test_mae}')


# Random Forest
# Create and train the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions
train_predictions = rf_model.predict(X_train)
valid_predictions = rf_model.predict(X_valid)
test_predictions = rf_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (Random Forest): {train_mse}')
print(f'Validation Mean Squared Error (Random Forest): {valid_mse}')
print(f'Testing Mean Squared Error (Random Forest): {test_mse}')
print(f'Training Mean Absolute Error (Random Forest): {train_mae}')
print(f'Validation Mean Absolute Error (Random Forest): {valid_mae}')
print(f'Testing Mean Absolute Error (Random Forest): {test_mae}')


# ANN
# Create the ANN model
model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dense(32, activation='relu'),
    layers.Dense(1)  # Output layer for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_valid, y_valid))

# Evaluate the model
loss, mae = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}')
print(f'Test Mean Absolute Error: {mae}')

# Make predictions
train_predictions = model.predict(X_train).flatten()
valid_predictions = model.predict(X_valid).flatten()
test_predictions = model.predict(X_test).flatten()

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (ANN): {train_mse}')
print(f'Validation Mean Squared Error (ANN): {valid_mse}')
print(f'Testing Mean Squared Error (ANN): {test_mse}')
print(f'Training Mean Absolute Error (ANN): {train_mae}')
print(f'Validation Mean Absolute Error (ANN): {valid_mae}')
print(f'Testing Mean Absolute Error (ANN): {test_mae}')


# SVM
# Create and train the SVM model
svm_model = SVR(kernel='rbf')  # You can experiment with different kernels
svm_model.fit(X_train, y_train)

# Make predictions
train_predictions = svm_model.predict(X_train)
valid_predictions = svm_model.predict(X_valid)
test_predictions = svm_model.predict(X_test)

# Calculate training, validation, and testing errors
train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

# Print the results
print(f'Training Mean Squared Error (SVM): {train_mse}')
print(f'Validation Mean Squared Error (SVM): {valid_mse}')
print(f'Testing Mean Squared Error (SVM): {test_mse}')
print(f'Training Mean Absolute Error (SVM): {train_mae}')
print(f'Validation Mean Absolute Error (SVM): {valid_mae}')
print(f'Testing Mean Absolute Error (SVM): {test_mae}')

# prompt: plot training and testing result

import matplotlib.pyplot as plt

# Assuming you have your test predictions and true test values in the variables 'test_predictions' and 'y_test'
plt.figure(figsize=(8, 6))
plt.scatter(y_test, test_predictions, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)  # Diagonal line for perfect prediction
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.title('True vs. Predicted Values (Test Set)')
plt.grid(True)
plt.show()

# Plot training and validation loss
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

# prompt: tune the above model

import xgboost as xgb
import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split, GridSearchCV
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.svm import SVR

# XGBoost Hyperparameter Tuning
X = data[['DBH_m_', 'VH_mean']]
y = data['canopy_cir']

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)
dtest = xgb.DMatrix(X_test, label=y_test)

param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [100, 200, 300],
    'subsample': [0.8, 1.0],  # Example subsample parameter
    'colsample_bytree': [0.8, 1.0]  # Example colsample_bytree parameter
}

xgb_model = xgb.XGBRegressor(objective='reg:squarederror', seed=42)
grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid,
                           scoring='neg_mean_squared_error', cv=3, verbose=1)
grid_search.fit(X_train, y_train)

print("Best parameters found: ", grid_search.best_params_)

# Train with the best parameters
best_xgb_model = xgb.XGBRegressor(**grid_search.best_params_, objective='reg:squarederror', seed=42)
best_xgb_model.fit(X_train, y_train)

# Evaluate the tuned model
train_predictions = best_xgb_model.predict(X_train)
valid_predictions = best_xgb_model.predict(X_valid)
test_predictions = best_xgb_model.predict(X_test)

train_mse = mean_squared_error(y_train, train_predictions)
valid_mse = mean_squared_error(y_valid, valid_predictions)
test_mse = mean_squared_error(y_test, test_predictions)

train_mae = mean_absolute_error(y_train, train_predictions)
valid_mae = mean_absolute_error(y_valid, valid_predictions)
test_mae = mean_absolute_error(y_test, test_predictions)

print(f'Training Mean Squared Error (Tuned XGBoost): {train_mse}')
print(f'Validation Mean Squared Error (Tuned XGBoost): {valid_mse}')
print(f'Testing Mean Squared Error (Tuned XGBoost): {test_mse}')
print(f'Training Mean Absolute Error (Tuned XGBoost): {train_mae}')
print(f'Validation Mean Absolute Error (Tuned XGBoost): {valid_mae}')
print(f'Testing Mean Absolute Error (Tuned XGBoost): {test_mae}')